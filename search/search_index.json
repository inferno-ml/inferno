{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Getting Started","text":"Blazingly-fast Bayesian Deep Learning in PyTorch."},{"location":"#setup","title":"Setup","text":"<p>You can install inferno using <code>pip</code>:</p> <pre><code>pip install inferno-torch\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import inferno\n</code></pre>"},{"location":"CONTRIBUTING/","title":"Contribution Guidelines","text":""},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>To get started with contributions to the library follow these steps:</p> <ol> <li>Fork the repository.</li> <li>Clone your fork to your machine:     <pre><code>git clone git@github.com:my-username/inferno.git\n</code></pre></li> <li>Install the Python package in an editable fashion with all development dependencies (in a new virtual environment):     <pre><code>cd inferno\npip install -e .[dev]\n</code></pre></li> </ol> <p>That's it! To make contributions to the main library simply push your changes to your fork on GitHub and create a pull request.</p>"},{"location":"CONTRIBUTING/#continuous-integration","title":"Continuous Integration","text":"<p>We use <code>tox</code> to simplify any tasks related to continuous integration.</p>"},{"location":"CONTRIBUTING/#running-tests","title":"Running Tests","text":"<p>You can run (a subset of) tests via <pre><code>tox -e py3 -- path-to-tests\n</code></pre></p>"},{"location":"CONTRIBUTING/#formatting-code","title":"Formatting Code","text":"<p>To ensure consistent formatting throughout the library, we check for properly formatted code and imports. </p> <p>To check whether your code is properly formatted run:</p> <pre><code>tox -e format -- . --check --diff\n</code></pre> <p>If you want <code>tox</code> to automatically format a specific folder, simply run <pre><code>tox -e format -- folder-to-format\n</code></pre></p>"},{"location":"CONTRIBUTING/#building-the-documentation","title":"Building the Documentation","text":"<p>You can build the documentation and view it locally in your browser.</p> <pre><code>tox -e docs -- serve \n</code></pre> <p>Now open a browser and enter the local address shown in your terminal.</p>"},{"location":"CONTRIBUTING/#running-code-examples","title":"Running Code Examples","text":"<p>You can run the code examples in the documentation, like so.</p> <pre><code># Run all examples\ntox -e examples\n\n# Run a specific example\ntox -e examples -- docs/examples/my_example/run.py \n</code></pre>"},{"location":"api/loss_fns/","title":"<code class=\"doc-symbol doc-symbol-heading\">loss_fns</code>","text":""},{"location":"api/loss_fns/#inferno.loss_fns","title":"loss_fns","text":"<p>Loss functions.</p> <p>Classes:</p> Name Description <code>BCELoss</code> <code>BCEWithLogitsLoss</code> <code>BCEWithLogitsLossVR</code> <p>Binary Cross Entropy Loss with reduced variance for models with stochastic parameters.</p> <code>CrossEntropyLoss</code> <code>CrossEntropyLossVR</code> <p>Cross Entropy Loss with reduced variance for models with stochastic parameters.</p> <code>FocalLoss</code> <p>The focal loss rescales the cross entropy loss with a factor that induces a regularizer on the output class probabilities.</p> <code>L1Loss</code> <code>MSELoss</code> <code>MSELossVR</code> <p>Mean-Squared Error Loss with reduced variance for models with stochastic parameters.</p> <code>NLLLoss</code> <code>VariationalFreeEnergy</code> <p>Variational Free Energy Loss.</p> <p>Attributes:</p> Name Type Description <code>NegativeELBO</code>"},{"location":"api/loss_fns/#inferno.loss_fns.NegativeELBO","title":"NegativeELBO","text":"<pre><code>NegativeELBO = VariationalFreeEnergy\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.BCELoss","title":"BCELoss","text":"<p>               Bases: <code>MultipleBatchDimensionsLossMixin</code>, <code>BCELoss</code></p> <p>Methods:</p> Name Description <code>forward</code>"},{"location":"api/loss_fns/#inferno.loss_fns.BCELoss.forward","title":"forward","text":"<pre><code>forward(pred: Tensor, target: Tensor) -&gt; Tensor\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.BCEWithLogitsLoss","title":"BCEWithLogitsLoss","text":"<pre><code>BCEWithLogitsLoss(\n    weight: Tensor | None = None,\n    reduction: Literal[\"mean\", \"sum\", \"none\"] = \"mean\",\n    pos_weight: Tensor | None = None,\n)\n</code></pre> <p>               Bases: <code>MultipleBatchDimensionsLossMixin</code>, <code>BCEWithLogitsLoss</code></p> <p>Methods:</p> Name Description <code>forward</code>"},{"location":"api/loss_fns/#inferno.loss_fns.BCEWithLogitsLoss.forward","title":"forward","text":"<pre><code>forward(pred: Tensor, target: Tensor) -&gt; Tensor\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.BCEWithLogitsLossVR","title":"BCEWithLogitsLossVR","text":"<pre><code>BCEWithLogitsLossVR(\n    reduction: Literal[\"none\", \"sum\", \"mean\"] = \"mean\",\n)\n</code></pre> <p>               Bases: <code>_Loss</code></p> <p>Binary Cross Entropy Loss with reduced variance for models with stochastic parameters.</p> <p>The loss on a single datapoint is given by</p> \\[     \\begin{align*}         \\ell_n &amp;= \\mathbb{E}_{w}[-\\log p(y_n \\mid f_w(x_n))]\\\\                &amp;= -\\mathbb{E}_{w}[y_n \\log \\sigma(f_w(x_n)) + (1 - y_n) \\log \\sigma(-f_w(x_n))]\\\\                &amp;\\leq \\mathbb{E}_{w_{1:L-1}}\\big[y_n \\log(1+\\mathbb{E}_{w_L \\mid w_{1:L-1}}[\\exp(-f_w(x_n))])\\\\                 &amp;\\qquad+ (1 - y_n) \\log(1+\\mathbb{E}_{w_L \\mid w_{1:L-1}}[\\exp(f_w(x_n))])\\big]     \\end{align*} \\] <p>which for a linear Gaussian output layer equals</p> \\[     \\begin{align*}         \\ell_n &amp;\\leq \\mathbb{E}_{w_{1:L-1}}\\big[ y_n \\big(\\log(1 + \\exp(\\mathbb{E}_{w_L \\mid w_{1:L-1}}[-f_w(x_n)] + \\frac{1}{2}\\operatorname{Var}_{w_L \\mid w_{1:L-1}}[f_w(x_n)]) \\big) \\\\                &amp;\\qquad+ (1-y_n) \\big(\\log(1 + \\exp(\\mathbb{E}_{w_L \\mid w_{1:L-1}}[f_w(x_n)] + \\frac{1}{2}\\operatorname{Var}_{w_L \\mid w_{1:L-1}}[f_w(x_n)])\\big)\\big].     \\end{align*} \\] <p>which defines an upper bound on the expected value of the cross entropy loss.  For models with stochastic parameters, this loss has lower variance in exchange for bias compared to <code>inferno.loss_fns.CrossEntropyLoss</code>,  which directly computes a Monte-Carlo approximation of the expected loss .</p> <p>The <code>reduction</code> is applied over all sample and batch dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['none', 'sum', 'mean']</code> <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the weighted mean of the output is taken, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Methods:</p> Name Description <code>forward</code> <p>Runs the forward pass.</p>"},{"location":"api/loss_fns/#inferno.loss_fns.BCEWithLogitsLossVR.forward","title":"forward","text":"<pre><code>forward(\n    input_representation: Float[\n        Tensor, \"*sample batch *feature\"\n    ],\n    output_layer: BNNMixin,\n    target: Float[Tensor, \"batch *out_feature\"],\n)\n</code></pre> <p>Runs the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>input_representation</code> <code>Float[Tensor, '*sample batch *feature']</code> <p>(Penultimate layer) representation of input tensor. This is the representation produced by a forward pass through all hidden layers, which will be fed as inputs to the output layer in a forward pass.</p> required <code>output_layer</code> <code>BNNMixin</code> <p>Output layer of the model.</p> required <code>target</code> <code>Float[Tensor, 'batch *out_feature']</code> <p>Target tensor.</p> required"},{"location":"api/loss_fns/#inferno.loss_fns.CrossEntropyLoss","title":"CrossEntropyLoss","text":"<p>               Bases: <code>MultipleBatchDimensionsLossMixin</code>, <code>CrossEntropyLoss</code></p> <p>Methods:</p> Name Description <code>forward</code>"},{"location":"api/loss_fns/#inferno.loss_fns.CrossEntropyLoss.forward","title":"forward","text":"<pre><code>forward(pred: Tensor, target: Tensor) -&gt; Tensor\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.CrossEntropyLossVR","title":"CrossEntropyLossVR","text":"<pre><code>CrossEntropyLossVR(\n    reduction: Literal[\"none\", \"sum\", \"mean\"] = \"mean\",\n)\n</code></pre> <p>               Bases: <code>_Loss</code></p> <p>Cross Entropy Loss with reduced variance for models with stochastic parameters.</p> <p>The loss on a single datapoint is given by</p> \\[     \\begin{align*}         \\ell_n &amp;= \\mathbb{E}_{w}[-\\log p(y_n \\mid f_w(x_n))]\\\\                &amp;= \\mathbb{E}_{w}[-\\log \\operatorname{softmax}(f_w(x_n))_{y_n}]\\\\                &amp;\\leq \\mathbb{E}_{w_{1:L-1}}\\bigg[\\mathbb{E}_{w_L \\mid w_{1:L-1}}[-f_w(x_n)_{y_n}]                     + \\log \\sum_{c=1}^C \\mathbb{E}_{w_L \\mid w_{1:L-1}}[\\exp(f_w(x_n)_{c})]\\bigg],     \\end{align*} \\] <p>which for a linear Gaussian output layer equals</p> \\[     \\begin{equation*}         \\ell_n \\leq \\mathbb{E}_{w_{1:L-1}}\\bigg[\\mathbb{E}_{w_L \\mid w_{1:L-1}}[-f_w(x_n)_{y_n}]                     + \\operatorname{logsumexp}\\big(\\mathbb{E}_{w_L \\mid w_{1:L-1}}[f_w(x_n)_{c}]                      + \\frac{1}{2}\\operatorname{Var}_{w_L \\mid w_{1:L-1}}[f_w(x_n)_c]\\big)\\bigg].     \\end{equation*} \\] <p>This loss defines an upper bound on the expected value of the cross entropy loss and for models with  stochastic parameters has lower variance in exchange for bias compared to <code>inferno.loss_fns.CrossEntropyLoss</code>,  which directly computes a Monte-Carlo approximation of the expected loss.</p> <p>The <code>reduction</code> is applied over all sample and batch dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['none', 'sum', 'mean']</code> <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the weighted mean of the output is taken, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Methods:</p> Name Description <code>forward</code> <p>Runs the forward pass.</p>"},{"location":"api/loss_fns/#inferno.loss_fns.CrossEntropyLossVR.forward","title":"forward","text":"<pre><code>forward(\n    input_representation: Float[\n        Tensor, \"*sample batch *feature\"\n    ],\n    output_layer: BNNMixin,\n    target: Float[Tensor, \"batch *out_feature\"],\n)\n</code></pre> <p>Runs the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>input_representation</code> <code>Float[Tensor, '*sample batch *feature']</code> <p>(Penultimate layer) representation of input tensor. This is the representation produced by a forward pass through all hidden layers, which will be fed as inputs to the output layer in a forward pass.</p> required <code>output_layer</code> <code>BNNMixin</code> <p>Output layer of the model.</p> required <code>target</code> <code>Float[Tensor, 'batch *out_feature']</code> <p>Target tensor.</p> required"},{"location":"api/loss_fns/#inferno.loss_fns.FocalLoss","title":"FocalLoss","text":"<pre><code>FocalLoss(\n    task: Literal[\"binary\", \"multiclass\"],\n    gamma: float = 2.0,\n    num_classes: int | None = None,\n    weight: Tensor | None = None,\n    reduction: Literal[\"none\", \"sum\", \"mean\"] = \"mean\",\n)\n</code></pre> <p>               Bases: <code>_WeightedLoss</code></p> <p>The focal loss rescales the cross entropy loss with a factor that induces a regularizer on the output class probabilities.</p> <p>The focal loss is useful to address class imbalance (Lin et al. 2017) and to improve calibration (Mukhoti et al. 2020). The loss on a single datapoint is given by</p> \\[     \\begin{equation*}     \\ell_n = -(1-\\hat{p}_{y_n})^\\gamma\\log \\hat{p}_{y_n}.     \\end{equation*} \\] <p>For \\(\\gamma=1\\) the focal loss equals the cross entropy loss with an entropic regularizer on the predicted class probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Literal['binary', 'multiclass']</code> <p>Specifies the type of task: 'binary' or 'multiclass'.</p> required <code>gamma</code> <code>float</code> <p>Focusing parameter, controls the strength of the modulating factor \\((1-\\hat{p}_{y_n})^\\gamma\\).</p> <code>2.0</code> <code>num_classes</code> <code>int | None</code> <p>Number of classes (only required for multi-class classification)</p> <code>None</code> <code>weight</code> <code>Tensor | None</code> <p>A manual rescaling weight given to each class. If given, has to be a Tensor of size C.</p> <code>None</code> <code>reduction</code> <code>Literal['none', 'sum', 'mean']</code> <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the weighted mean of the output is taken, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>ce_loss_fn</code> <code>gamma</code> <code>num_classes</code> <code>task</code>"},{"location":"api/loss_fns/#inferno.loss_fns.FocalLoss.ce_loss_fn","title":"ce_loss_fn","text":"<pre><code>ce_loss_fn = BCEWithLogitsLoss(\n    weight=weight, reduction=\"none\"\n)\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.FocalLoss.gamma","title":"gamma","text":"<pre><code>gamma = gamma\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.FocalLoss.num_classes","title":"num_classes","text":"<pre><code>num_classes = num_classes\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.FocalLoss.task","title":"task","text":"<pre><code>task = task\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.FocalLoss.forward","title":"forward","text":"<pre><code>forward(pred: Tensor, target: Tensor)\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.L1Loss","title":"L1Loss","text":"<p>               Bases: <code>MultipleBatchDimensionsLossMixin</code>, <code>L1Loss</code></p> <p>Methods:</p> Name Description <code>forward</code>"},{"location":"api/loss_fns/#inferno.loss_fns.L1Loss.forward","title":"forward","text":"<pre><code>forward(pred: Tensor, target: Tensor) -&gt; Tensor\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.MSELoss","title":"MSELoss","text":"<p>               Bases: <code>MultipleBatchDimensionsLossMixin</code>, <code>MSELoss</code></p> <p>Methods:</p> Name Description <code>forward</code>"},{"location":"api/loss_fns/#inferno.loss_fns.MSELoss.forward","title":"forward","text":"<pre><code>forward(pred: Tensor, target: Tensor) -&gt; Tensor\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.MSELossVR","title":"MSELossVR","text":"<pre><code>MSELossVR(\n    reduction: Literal[\"none\", \"sum\", \"mean\"] = \"mean\",\n)\n</code></pre> <p>               Bases: <code>_Loss</code></p> <p>Mean-Squared Error Loss with reduced variance for models with stochastic parameters.</p> <p>The loss on a single datapoint is given by</p> \\[     \\begin{align*}         \\ell_n &amp;= \\mathbb{E}_{w}[(f_w(x_n) - y_n)^2]\\\\                &amp;= \\mathbb{E}_{w_{1:L-1}}\\big[\\mathbb{E}_{w_L \\mid w_{1:L-1}}[(f_w(x_n) - y_n)^2]\\big]\\\\                &amp;= \\mathbb{E}_{w_{1:L-1}}\\big[(\\mathbb{E}_{w_L \\mid w_{1:L-1}}[f_w(x_n)] - y_n)^2                      + \\operatorname{Var}_{w_L \\mid w_{1:L-1}}[f_w(x_n)]\\big].     \\end{align*} \\] <p>For models with stochastic parameters, the conditional Monte-Carlo estimate results in variance reduction compared to using <code>inferno.loss_fns.MSELoss</code> which directly computes a Monte-Carlo approximation of the expected loss.</p> <p>The <code>reduction</code> is applied over all sample and batch dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['none', 'sum', 'mean']</code> <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the weighted mean of the output is taken, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Methods:</p> Name Description <code>forward</code> <p>Runs the forward pass.</p>"},{"location":"api/loss_fns/#inferno.loss_fns.MSELossVR.forward","title":"forward","text":"<pre><code>forward(\n    input_representation: Float[\n        Tensor, \"*sample batch *feature\"\n    ],\n    output_layer: BNNMixin,\n    target: Float[Tensor, \"batch *out_feature\"],\n)\n</code></pre> <p>Runs the forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>input_representation</code> <code>Float[Tensor, '*sample batch *feature']</code> <p>(Penultimate layer) representation of input tensor. This is the representation produced by a forward pass through all hidden layers, which will be fed as inputs to the output layer in a forward pass.</p> required <code>output_layer</code> <code>BNNMixin</code> <p>Output layer of the model.</p> required <code>target</code> <code>Float[Tensor, 'batch *out_feature']</code> <p>Target tensor.</p> required"},{"location":"api/loss_fns/#inferno.loss_fns.NLLLoss","title":"NLLLoss","text":"<p>               Bases: <code>MultipleBatchDimensionsLossMixin</code>, <code>NLLLoss</code></p> <p>Methods:</p> Name Description <code>forward</code>"},{"location":"api/loss_fns/#inferno.loss_fns.NLLLoss.forward","title":"forward","text":"<pre><code>forward(pred: Tensor, target: Tensor) -&gt; Tensor\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy","title":"VariationalFreeEnergy","text":"<pre><code>VariationalFreeEnergy(\n    nll: _Loss,\n    model: BNNMixin,\n    prior_loc: Float[Tensor, \"parameter\"] | None = None,\n    prior_scale: Float[Tensor, \"parameter\"] | None = None,\n    kl_weight: float | None = 1.0,\n    reduction: str = \"mean\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Variational Free Energy Loss.</p> <p>Computes the variational free energy loss for variational inference with the Kullback-Leibler regularization term computed in weight space. This is also known as the negative evidence lower bound (ELBO).</p> <p>Parameters:</p> Name Type Description Default <code>nll</code> <code>_Loss</code> <p>Loss function defining the negative log-likelihood.</p> required <code>model</code> <code>BNNMixin</code> <p>The probabilistic model.</p> required <code>prior_loc</code> <code>Float[Tensor, 'parameter'] | None</code> <p>Location(s) of the prior Gaussian distribution.</p> <code>None</code> <code>prior_scale</code> <code>Float[Tensor, 'parameter'] | None</code> <p>Scale(s) of the prior Gaussian distribution.</p> <code>None</code> <code>kl_weight</code> <code>float | None</code> <p>Weight for the KL divergence term. If <code>None</code>, chooses the weight inversely proportional to the number of mean parameters.</p> <code>1.0</code> <code>reduction</code> <code>str</code> <p>Specifies the reduction to apply to the output: <code>``'mean'</code> | <code>'sum'</code>. <code>'mean'</code>: the weighted mean of the output is taken, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>kl_weight</code> <code>model</code> <code>nll</code> <code>numel_mean_parameters</code> <code>prior_loc</code> <code>prior_scale</code> <code>reduction</code>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.kl_weight","title":"kl_weight","text":"<pre><code>kl_weight = kl_weight\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.model","title":"model","text":"<pre><code>model = model\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.nll","title":"nll","text":"<pre><code>nll = nll\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.numel_mean_parameters","title":"numel_mean_parameters","text":"<pre><code>numel_mean_parameters = sum(\n    (numel())\n    for name, param in (named_parameters())\n    if requires_grad\n    and \"params.\" in name\n    and \"cov.\" not in name\n)\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.prior_loc","title":"prior_loc","text":"<pre><code>prior_loc = prior_loc\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.prior_scale","title":"prior_scale","text":"<pre><code>prior_scale = prior_scale\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.reduction","title":"reduction","text":"<pre><code>reduction = reduction\n</code></pre>"},{"location":"api/loss_fns/#inferno.loss_fns.VariationalFreeEnergy.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch in_feature\"],\n    target: Float[Tensor, \"batch out_feature\"],\n) -&gt; Float[Tensor, \"\"]\n</code></pre>"},{"location":"api/models/","title":"<code class=\"doc-symbol doc-symbol-heading\">models</code>","text":""},{"location":"api/models/#inferno.models","title":"models","text":"<p>Pre-defined models.</p> <p>Classes:</p> Name Description <code>Ensemble</code> <p>An ensemble of models.</p> <code>LeNet5</code> <p>A simple convolutional neural network for image classification of 28x28 grayscale images.</p> <code>MLP</code> <p>A fully-connected feedforward neural network with the same activation function in each layer.</p> <code>ResNeXt101_32X8D</code> <p>ResNext-101 (32x8d)</p> <code>ResNeXt101_64X4D</code> <p>ResNext-101 (32x4d)</p> <code>ResNeXt50_32X4D</code> <p>ResNext-50 (32x4d)</p> <code>ResNet</code> <p>A residual neural network for image classification.</p> <code>ResNet101</code> <p>ResNet-101</p> <code>ResNet18</code> <p>ResNet-18</p> <code>ResNet34</code> <p>ResNet-34</p> <code>ResNet50</code> <p>ResNet-50</p> <code>ViT_B_16</code> <p>ViT_B_16</p> <code>ViT_B_32</code> <p>ViT_B_32</p> <code>ViT_H_14</code> <p>ViT_H_14</p> <code>ViT_L_16</code> <p>ViT_L_16</p> <code>ViT_L_32</code> <p>ViT_L_32</p> <code>VisionTransformer</code> <p>Vision Transformer as per https://arxiv.org/abs/2010.11929.</p> <code>WideResNet101</code> <p>WideResNet-101-2</p> <code>WideResNet50</code> <p>WideResNet-50-2</p> <p>Attributes:</p> Name Type Description <code>___all__</code>"},{"location":"api/models/#inferno.models.___all__","title":"___all__","text":"<pre><code>___all__ = [\n    \"Ensemble\",\n    \"LeNet5\",\n    \"MLP\",\n    \"ResNet\",\n    \"ResNet18\",\n    \"ResNet34\",\n    \"ResNet50\",\n    \"ResNet101\",\n    \"ResNeXt50_32X4D\",\n    \"ResNeXt101_32X8D\",\n    \"ResNeXt101_64X4D\",\n    \"WideResNet50\",\n    \"WideResNet101\",\n    \"as_torch_model\",\n]\n</code></pre>"},{"location":"api/models/#inferno.models.Ensemble","title":"Ensemble","text":"<pre><code>Ensemble(members: Iterable[Module])\n</code></pre> <p>               Bases: <code>BNNMixin</code>, <code>Module</code></p> <p>An ensemble of models.</p> <p>This class ensembles multiple models with the same architecture by averaging their predictions.</p> <p>Parameters:</p> Name Type Description Default <code>members</code> <code>Iterable[Module]</code> <p>List of models to ensemble.</p> required <p>Methods:</p> Name Description <code>forward</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>base_module</code> <code>members</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.Ensemble.base_module","title":"base_module","text":"<pre><code>base_module = [to(device='meta')]\n</code></pre>"},{"location":"api/models/#inferno.models.Ensemble.members","title":"members","text":"<pre><code>members = ModuleList(members)\n</code></pre>"},{"location":"api/models/#inferno.models.Ensemble.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.Ensemble.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*batch in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.Ensemble.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.Ensemble.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.LeNet5","title":"LeNet5","text":"<pre><code>LeNet5(\n    out_size: int = 10,\n    parametrization: Parametrization = MaximalUpdate(),\n    cov: FactorizedCovariance | None = None,\n    activation_layer: Callable[..., Module] | None = ReLU,\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>A simple convolutional neural network for image classification of 28x28 grayscale images.</p> <p>Parameters:</p> Name Type Description Default <code>out_size</code> <code>int</code> <p>Size of the output (i.e. number of classes).</p> <code>10</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>cov</code> <code>FactorizedCovariance | None</code> <p>Covariance structure of the weights.</p> <code>None</code> <code>activation_layer</code> <code>Callable[..., Module] | None</code> <p>Activation function following a linear layer.</p> <code>ReLU</code> <p>Methods:</p> Name Description <code>forward</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>out_size</code> <code>parametrization</code> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.LeNet5.out_size","title":"out_size","text":"<pre><code>out_size = out_size\n</code></pre>"},{"location":"api/models/#inferno.models.LeNet5.parametrization","title":"parametrization","text":"<pre><code>parametrization = parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.LeNet5.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*batch in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.LeNet5.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.LeNet5.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.MLP","title":"MLP","text":"<pre><code>MLP(\n    in_size: int,\n    hidden_sizes: list[int],\n    out_size: int | Size,\n    norm_layer: Callable[..., Module] | None = None,\n    activation_layer: Callable[..., Module] | None = ReLU,\n    inplace: bool | None = None,\n    bias: bool = True,\n    dropout: float | None = None,\n    parametrization: Parametrization = MaximalUpdate(),\n    cov: (\n        FactorizedCovariance\n        | list[FactorizedCovariance]\n        | None\n    ) = None,\n)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>A fully-connected feedforward neural network with the same activation function in each layer.</p> <p>Parameters:</p> Name Type Description Default <code>in_size</code> <code>int</code> <p>Size of the input.</p> required <code>hidden_sizes</code> <code>list[int]</code> <p>List of hidden layer sizes.</p> required <code>out_size</code> <code>int | Size</code> <p>Size of the output (e.g. number of classes).</p> required <code>norm_layer</code> <code>Callable[..., Module] | None</code> <p>Normalization layer which will be stacked on top of the linear layer.</p> <code>None</code> <code>activation_layer</code> <code>Callable[..., Module] | None</code> <p>Activation function following a linear layer.</p> <code>ReLU</code> <code>inplace</code> <code>bool | None</code> <p>Whether to apply the activation function and dropout inplace. Default is <code>None</code>, which uses the respective default values.</p> <code>None</code> <code>bias</code> <code>bool</code> <p>Whether to use bias in the linear layer.``</p> <code>True</code> <code>dropout</code> <code>float | None</code> <p>The probability for the dropout layer.</p> <code>None</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>cov</code> <code>FactorizedCovariance | list[FactorizedCovariance] | None</code> <p>Covariance structure of the weights.</p> <code>None</code> <p>Methods:</p> Name Description <code>forward</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>hidden_sizes</code> <code>in_size</code> <code>out_size</code> <code>parametrization</code> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.MLP.hidden_sizes","title":"hidden_sizes","text":"<pre><code>hidden_sizes = hidden_sizes\n</code></pre>"},{"location":"api/models/#inferno.models.MLP.in_size","title":"in_size","text":"<pre><code>in_size = in_size\n</code></pre>"},{"location":"api/models/#inferno.models.MLP.out_size","title":"out_size","text":"<pre><code>out_size = out_size\n</code></pre>"},{"location":"api/models/#inferno.models.MLP.parametrization","title":"parametrization","text":"<pre><code>parametrization = parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.MLP.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*batch in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.MLP.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.MLP.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D","title":"ResNeXt101_32X8D","text":"<pre><code>ResNeXt101_32X8D(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>ResNext-101 (32x8d)</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to <code>ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=deepcopy(cov),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=deepcopy(cov),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNeXt101_32X8D.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D","title":"ResNeXt101_64X4D","text":"<pre><code>ResNeXt101_64X4D(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>ResNext-101 (32x4d)</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to <code>ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=deepcopy(cov),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=deepcopy(cov),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNeXt101_64X4D.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D","title":"ResNeXt50_32X4D","text":"<pre><code>ResNeXt50_32X4D(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>ResNext-50 (32x4d)</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to <code>ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=deepcopy(cov),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=deepcopy(cov),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNeXt50_32X4D.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNet","title":"ResNet","text":"<pre><code>ResNet(\n    out_size: int,\n    block: type[\"BasicBlock\"] | type[\"Bottleneck\"],\n    num_blocks_per_layer: Sequence[int],\n    zero_init_residual: bool = False,\n    groups: int = 1,\n    width_per_group: int = 64,\n    replace_stride_with_dilation: Sequence[bool] = (\n        False,\n        False,\n        False,\n    ),\n    norm_layer: Callable[..., Module] = lambda c: GroupNorm(\n        num_groups=32, num_channels=c\n    ),\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    parametrization: Parametrization = MaximalUpdate(),\n    cov: FactorizedCovariance | None = None,\n)\n</code></pre> <p>               Bases: <code>BNNMixin</code>, <code>Module</code></p> <p>A residual neural network for image classification.</p> <p>Parameters:</p> Name Type Description Default <code>out_size</code> <code>int</code> <p>Size of the output (i.e. number of classes).</p> required <code>block</code> <code>type['BasicBlock'] | type['Bottleneck']</code> <p>Block type to use.</p> required <code>num_blocks_per_layer</code> <code>Sequence[int]</code> <p>Number of blocks per layer.</p> required <code>zero_init_residual</code> <code>bool</code> <code>False</code> <code>groups</code> <code>int</code> <p>Number of groups for the convolutional layers.</p> <code>1</code> <code>width_per_group</code> <code>int</code> <p>Width per group for the convolutional layers.</p> <code>64</code> <code>replace_stride_with_dilation</code> <code>Sequence[bool]</code> <p>Whether to replace the 2x2 stride with a dilated convolution. Must be a tuple of length 3.</p> <code>(False, False, False)</code> <code>norm_layer</code> <code>Callable[..., Module]</code> <p>Normalization layer to use.</p> <code>lambda c: GroupNorm(num_groups=32, num_channels=c)</code> <code>architecture</code> <code>Literal['imagenet', 'cifar']</code> <p>Type of ResNet architecture. Either \"imagenet\" or \"cifar\".</p> <code>'imagenet'</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>cov</code> <code>FactorizedCovariance | None</code> <p>Covariance structure of the probabilistic layers.</p> <code>None</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <p>Load a ResNet model with pretrained weights.</p> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNet.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=deepcopy(cov),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=deepcopy(cov),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNet.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    weights: Weights,\n    freeze: bool = False,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    *args,\n    **kwargs\n)\n</code></pre> <p>Load a ResNet model with pretrained weights.</p> <p>Depending on the <code>out_size</code> and <code>architecture</code> parameters, the first and last layers of the model are not initialized with the pretrained weights.</p> <p>Parameters:</p> Name Type Description Default <code>out_size</code> <code>int</code> <p>Size of the output (i.e. number of classes).</p> required <code>weights</code> <code>Weights</code> <p>Pretrained weights to use.</p> required <code>freeze</code> <code>bool</code> <p>Whether to freeze the pretrained weights.</p> <code>False</code> <code>architecture</code> <code>Literal['imagenet', 'cifar']</code> <p>Type of ResNet architecture. Either \"imagenet\" or \"cifar\".</p> <code>'imagenet'</code>"},{"location":"api/models/#inferno.models.ResNet.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNet.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNet101","title":"ResNet101","text":"<pre><code>ResNet101(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>ResNet-101</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to <code>ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNet101.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=deepcopy(cov),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=deepcopy(cov),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNet101.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet101.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNet101.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNet18","title":"ResNet18","text":"<pre><code>ResNet18(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>ResNet-18</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to <code>ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNet18.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=deepcopy(cov),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=deepcopy(cov),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNet18.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet18.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNet18.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNet34","title":"ResNet34","text":"<pre><code>ResNet34(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>ResNet-34</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to <code>ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNet34.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=deepcopy(cov),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=deepcopy(cov),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNet34.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet34.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNet34.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ResNet50","title":"ResNet50","text":"<pre><code>ResNet50(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>ResNet-50</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to <code>ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.ResNet50.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=deepcopy(cov),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=deepcopy(cov),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ResNet50.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ResNet50.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ResNet50.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.ViT_B_16","title":"ViT_B_16","text":"<pre><code>ViT_B_16(*args, **kwargs)\n</code></pre> <p>               Bases: <code>VisionTransformer</code></p> <p>ViT_B_16</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to <code>VisionTransformer</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>attention_dropout</code> <code>class_token</code> <code>conv_proj</code> <code>dropout</code> <code>encoder</code> <code>heads</code> <code>hidden_dim</code> <code>in_size</code> <code>mlp_dim</code> <code>norm_layer</code> <code>out_size</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>patch_size</code> <code>representation_size</code> <code>seq_length</code>"},{"location":"api/models/#inferno.models.ViT_B_16.attention_dropout","title":"attention_dropout","text":"<pre><code>attention_dropout = attention_dropout\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.class_token","title":"class_token","text":"<pre><code>class_token = Parameter(zeros(1, 1, hidden_dim))\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.conv_proj","title":"conv_proj","text":"<pre><code>conv_proj = Conv2d(\n    in_channels=3,\n    out_channels=hidden_dim,\n    kernel_size=patch_size,\n    stride=patch_size,\n    cov=cov[\"conv_proj\"],\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.dropout","title":"dropout","text":"<pre><code>dropout = dropout\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.encoder","title":"encoder","text":"<pre><code>encoder = Encoder(\n    seq_length,\n    num_layers,\n    num_heads,\n    hidden_dim,\n    mlp_dim,\n    dropout,\n    attention_dropout,\n    norm_layer,\n    parametrization=parametrization,\n    cov=cov[\"encoder\"],\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.heads","title":"heads","text":"<pre><code>heads = Sequential(heads_layers)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.hidden_dim","title":"hidden_dim","text":"<pre><code>hidden_dim = hidden_dim\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.in_size","title":"in_size","text":"<pre><code>in_size = in_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.mlp_dim","title":"mlp_dim","text":"<pre><code>mlp_dim = mlp_dim\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.norm_layer","title":"norm_layer","text":"<pre><code>norm_layer = norm_layer\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.out_size","title":"out_size","text":"<pre><code>out_size = out_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ViT_B_16.patch_size","title":"patch_size","text":"<pre><code>patch_size = patch_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.representation_size","title":"representation_size","text":"<pre><code>representation_size = representation_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.seq_length","title":"seq_length","text":"<pre><code>seq_length = seq_length\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.forward","title":"forward","text":"<pre><code>forward(\n    x: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    in_size: int,\n    out_size: int,\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_16.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Needs to be implemented because VisionTransformer has direct parameters.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ViT_B_16.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>Needs to be implemented because VisionTransformer has direct parameters.</p>"},{"location":"api/models/#inferno.models.ViT_B_32","title":"ViT_B_32","text":"<pre><code>ViT_B_32(*args, **kwargs)\n</code></pre> <p>               Bases: <code>VisionTransformer</code></p> <p>ViT_B_32</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to <code>VisionTransformer</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>attention_dropout</code> <code>class_token</code> <code>conv_proj</code> <code>dropout</code> <code>encoder</code> <code>heads</code> <code>hidden_dim</code> <code>in_size</code> <code>mlp_dim</code> <code>norm_layer</code> <code>out_size</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>patch_size</code> <code>representation_size</code> <code>seq_length</code>"},{"location":"api/models/#inferno.models.ViT_B_32.attention_dropout","title":"attention_dropout","text":"<pre><code>attention_dropout = attention_dropout\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.class_token","title":"class_token","text":"<pre><code>class_token = Parameter(zeros(1, 1, hidden_dim))\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.conv_proj","title":"conv_proj","text":"<pre><code>conv_proj = Conv2d(\n    in_channels=3,\n    out_channels=hidden_dim,\n    kernel_size=patch_size,\n    stride=patch_size,\n    cov=cov[\"conv_proj\"],\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.dropout","title":"dropout","text":"<pre><code>dropout = dropout\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.encoder","title":"encoder","text":"<pre><code>encoder = Encoder(\n    seq_length,\n    num_layers,\n    num_heads,\n    hidden_dim,\n    mlp_dim,\n    dropout,\n    attention_dropout,\n    norm_layer,\n    parametrization=parametrization,\n    cov=cov[\"encoder\"],\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.heads","title":"heads","text":"<pre><code>heads = Sequential(heads_layers)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.hidden_dim","title":"hidden_dim","text":"<pre><code>hidden_dim = hidden_dim\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.in_size","title":"in_size","text":"<pre><code>in_size = in_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.mlp_dim","title":"mlp_dim","text":"<pre><code>mlp_dim = mlp_dim\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.norm_layer","title":"norm_layer","text":"<pre><code>norm_layer = norm_layer\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.out_size","title":"out_size","text":"<pre><code>out_size = out_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ViT_B_32.patch_size","title":"patch_size","text":"<pre><code>patch_size = patch_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.representation_size","title":"representation_size","text":"<pre><code>representation_size = representation_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.seq_length","title":"seq_length","text":"<pre><code>seq_length = seq_length\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.forward","title":"forward","text":"<pre><code>forward(\n    x: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    in_size: int,\n    out_size: int,\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_B_32.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Needs to be implemented because VisionTransformer has direct parameters.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ViT_B_32.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>Needs to be implemented because VisionTransformer has direct parameters.</p>"},{"location":"api/models/#inferno.models.ViT_H_14","title":"ViT_H_14","text":"<pre><code>ViT_H_14(*args, **kwargs)\n</code></pre> <p>               Bases: <code>VisionTransformer</code></p> <p>ViT_H_14</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to <code>VisionTransformer</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>attention_dropout</code> <code>class_token</code> <code>conv_proj</code> <code>dropout</code> <code>encoder</code> <code>heads</code> <code>hidden_dim</code> <code>in_size</code> <code>mlp_dim</code> <code>norm_layer</code> <code>out_size</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>patch_size</code> <code>representation_size</code> <code>seq_length</code>"},{"location":"api/models/#inferno.models.ViT_H_14.attention_dropout","title":"attention_dropout","text":"<pre><code>attention_dropout = attention_dropout\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.class_token","title":"class_token","text":"<pre><code>class_token = Parameter(zeros(1, 1, hidden_dim))\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.conv_proj","title":"conv_proj","text":"<pre><code>conv_proj = Conv2d(\n    in_channels=3,\n    out_channels=hidden_dim,\n    kernel_size=patch_size,\n    stride=patch_size,\n    cov=cov[\"conv_proj\"],\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.dropout","title":"dropout","text":"<pre><code>dropout = dropout\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.encoder","title":"encoder","text":"<pre><code>encoder = Encoder(\n    seq_length,\n    num_layers,\n    num_heads,\n    hidden_dim,\n    mlp_dim,\n    dropout,\n    attention_dropout,\n    norm_layer,\n    parametrization=parametrization,\n    cov=cov[\"encoder\"],\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.heads","title":"heads","text":"<pre><code>heads = Sequential(heads_layers)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.hidden_dim","title":"hidden_dim","text":"<pre><code>hidden_dim = hidden_dim\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.in_size","title":"in_size","text":"<pre><code>in_size = in_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.mlp_dim","title":"mlp_dim","text":"<pre><code>mlp_dim = mlp_dim\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.norm_layer","title":"norm_layer","text":"<pre><code>norm_layer = norm_layer\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.out_size","title":"out_size","text":"<pre><code>out_size = out_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ViT_H_14.patch_size","title":"patch_size","text":"<pre><code>patch_size = patch_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.representation_size","title":"representation_size","text":"<pre><code>representation_size = representation_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.seq_length","title":"seq_length","text":"<pre><code>seq_length = seq_length\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.forward","title":"forward","text":"<pre><code>forward(\n    x: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    in_size: int,\n    out_size: int,\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_H_14.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Needs to be implemented because VisionTransformer has direct parameters.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ViT_H_14.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>Needs to be implemented because VisionTransformer has direct parameters.</p>"},{"location":"api/models/#inferno.models.ViT_L_16","title":"ViT_L_16","text":"<pre><code>ViT_L_16(*args, **kwargs)\n</code></pre> <p>               Bases: <code>VisionTransformer</code></p> <p>ViT_L_16</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to <code>VisionTransformer</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>attention_dropout</code> <code>class_token</code> <code>conv_proj</code> <code>dropout</code> <code>encoder</code> <code>heads</code> <code>hidden_dim</code> <code>in_size</code> <code>mlp_dim</code> <code>norm_layer</code> <code>out_size</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>patch_size</code> <code>representation_size</code> <code>seq_length</code>"},{"location":"api/models/#inferno.models.ViT_L_16.attention_dropout","title":"attention_dropout","text":"<pre><code>attention_dropout = attention_dropout\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.class_token","title":"class_token","text":"<pre><code>class_token = Parameter(zeros(1, 1, hidden_dim))\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.conv_proj","title":"conv_proj","text":"<pre><code>conv_proj = Conv2d(\n    in_channels=3,\n    out_channels=hidden_dim,\n    kernel_size=patch_size,\n    stride=patch_size,\n    cov=cov[\"conv_proj\"],\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.dropout","title":"dropout","text":"<pre><code>dropout = dropout\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.encoder","title":"encoder","text":"<pre><code>encoder = Encoder(\n    seq_length,\n    num_layers,\n    num_heads,\n    hidden_dim,\n    mlp_dim,\n    dropout,\n    attention_dropout,\n    norm_layer,\n    parametrization=parametrization,\n    cov=cov[\"encoder\"],\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.heads","title":"heads","text":"<pre><code>heads = Sequential(heads_layers)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.hidden_dim","title":"hidden_dim","text":"<pre><code>hidden_dim = hidden_dim\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.in_size","title":"in_size","text":"<pre><code>in_size = in_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.mlp_dim","title":"mlp_dim","text":"<pre><code>mlp_dim = mlp_dim\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.norm_layer","title":"norm_layer","text":"<pre><code>norm_layer = norm_layer\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.out_size","title":"out_size","text":"<pre><code>out_size = out_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ViT_L_16.patch_size","title":"patch_size","text":"<pre><code>patch_size = patch_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.representation_size","title":"representation_size","text":"<pre><code>representation_size = representation_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.seq_length","title":"seq_length","text":"<pre><code>seq_length = seq_length\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.forward","title":"forward","text":"<pre><code>forward(\n    x: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    in_size: int,\n    out_size: int,\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_16.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Needs to be implemented because VisionTransformer has direct parameters.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ViT_L_16.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>Needs to be implemented because VisionTransformer has direct parameters.</p>"},{"location":"api/models/#inferno.models.ViT_L_32","title":"ViT_L_32","text":"<pre><code>ViT_L_32(*args, **kwargs)\n</code></pre> <p>               Bases: <code>VisionTransformer</code></p> <p>ViT_L_32</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to <code>VisionTransformer</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>attention_dropout</code> <code>class_token</code> <code>conv_proj</code> <code>dropout</code> <code>encoder</code> <code>heads</code> <code>hidden_dim</code> <code>in_size</code> <code>mlp_dim</code> <code>norm_layer</code> <code>out_size</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>patch_size</code> <code>representation_size</code> <code>seq_length</code>"},{"location":"api/models/#inferno.models.ViT_L_32.attention_dropout","title":"attention_dropout","text":"<pre><code>attention_dropout = attention_dropout\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.class_token","title":"class_token","text":"<pre><code>class_token = Parameter(zeros(1, 1, hidden_dim))\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.conv_proj","title":"conv_proj","text":"<pre><code>conv_proj = Conv2d(\n    in_channels=3,\n    out_channels=hidden_dim,\n    kernel_size=patch_size,\n    stride=patch_size,\n    cov=cov[\"conv_proj\"],\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.dropout","title":"dropout","text":"<pre><code>dropout = dropout\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.encoder","title":"encoder","text":"<pre><code>encoder = Encoder(\n    seq_length,\n    num_layers,\n    num_heads,\n    hidden_dim,\n    mlp_dim,\n    dropout,\n    attention_dropout,\n    norm_layer,\n    parametrization=parametrization,\n    cov=cov[\"encoder\"],\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.heads","title":"heads","text":"<pre><code>heads = Sequential(heads_layers)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.hidden_dim","title":"hidden_dim","text":"<pre><code>hidden_dim = hidden_dim\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.in_size","title":"in_size","text":"<pre><code>in_size = in_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.mlp_dim","title":"mlp_dim","text":"<pre><code>mlp_dim = mlp_dim\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.norm_layer","title":"norm_layer","text":"<pre><code>norm_layer = norm_layer\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.out_size","title":"out_size","text":"<pre><code>out_size = out_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.ViT_L_32.patch_size","title":"patch_size","text":"<pre><code>patch_size = patch_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.representation_size","title":"representation_size","text":"<pre><code>representation_size = representation_size\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.seq_length","title":"seq_length","text":"<pre><code>seq_length = seq_length\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.forward","title":"forward","text":"<pre><code>forward(\n    x: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    in_size: int,\n    out_size: int,\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.ViT_L_32.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Needs to be implemented because VisionTransformer has direct parameters.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.ViT_L_32.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>Needs to be implemented because VisionTransformer has direct parameters.</p>"},{"location":"api/models/#inferno.models.VisionTransformer","title":"VisionTransformer","text":"<pre><code>VisionTransformer(\n    in_size: int,\n    patch_size: int,\n    num_layers: int,\n    num_heads: int,\n    hidden_dim: int,\n    mlp_dim: int,\n    dropout: float = 0.0,\n    attention_dropout: float = 0.0,\n    out_size: int = 1000,\n    representation_size: int | None = None,\n    norm_layer: Callable[..., Module] = partial(\n        LayerNorm, eps=1e-06\n    ),\n    conv_stem_configs: list[NamedTuple] | None = None,\n    parametrization: Parametrization = MaximalUpdate(),\n    cov: (\n        FactorizedCovariance\n        | dict[FactorizedCovariance]\n        | dict[dict[FactorizedCovariance]]\n        | None\n    ) = None,\n)\n</code></pre> <p>               Bases: <code>BNNMixin</code>, <code>Module</code></p> <p>Vision Transformer as per https://arxiv.org/abs/2010.11929.</p> <p>The covariance can be specified as <code>None</code> (resulting in a non-stochastic model), as an instance of <code>inferno.bnn.params.FactorizedCovariance</code> (resulting in the same covariance in all layers), or as a nested dictionary with the keys indicating the module. For example, the following will place a low rank covariance in the <code>conv_proj</code>, the last layer of the encoder, and the output head: <pre><code>cov = params.LowRankCovariance(rank=2)\nlast_layer_cov = {\n    \"conv_proj\": copy.deepcopy(cov),\n    \"encoder\": {\n        \"layers.encoder_layer_1\": {\n            \"self_attention\": {\n                \"q\": copy.deepcopy(cov),\n                \"k\": copy.deepcopy(cov),\n                \"v\": copy.deepcopy(cov),\n                \"out\": copy.deepcopy(cov),\n            },\n            \"mlp\": copy.deepcopy(cov)\n        },\n    },\n    \"heads.head\": copy.deepcopy(cov),\n}\nmodel = VisionTransformer(\n    in_size=32,\n    patch_size=16,\n    num_layers=2,\n    num_heads=2,\n    hidden_dim=10,\n    mlp_dim=10,\n    cov=last_layer_cov,\n)\n</code></pre> Note that any modules omitted from the covariance specification will default to <code>None</code> (in this example, any modules part of <code>last_layer_cov[\"encoder\"][\"layers.encoder_layer_0\"]</code>).</p> <p>Parameters:</p> Name Type Description Default <code>in_size</code> <code>int</code> <p>Size of the input (i.e. image size).</p> required <code>patch_size</code> <code>int</code> <p>Size of the patch.</p> required <code>num_layers</code> <code>int</code> <p>Number of layers in the encoder.</p> required <code>num_heads</code> <code>int</code> <p>Number of heads.</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden size in encoder.</p> required <code>mlp_dim</code> <code>int</code> <p>Dimension of MLP block.</p> required <code>dropout</code> <code>float</code> <p>Dropout probability.</p> <code>0.0</code> <code>attention_dropout</code> <code>float</code> <p>Attention dropout probability.</p> <code>0.0</code> <code>out_size</code> <code>int</code> <p>Size of the output (i.e. number of classes).</p> <code>1000</code> <code>representation_size</code> <code>int | None</code> <p>Size of pre-logits layer before output head.</p> <code>None</code> <code>norm_layer</code> <code>Callable[..., Module]</code> <p>Normalization layer to use.</p> <code>partial(LayerNorm, eps=1e-06)</code> <code>conv_stem_configs</code> <code>list[NamedTuple] | None</code> <p>Currently not supported.</p> <code>None</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>cov</code> <code>FactorizedCovariance | dict[FactorizedCovariance] | dict[dict[FactorizedCovariance]] | None</code> <p>Covariance structure of the probabilistic layers.</p> <code>None</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <p>Load a VisionTransformer model with pretrained weights.</p> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>attention_dropout</code> <code>class_token</code> <code>conv_proj</code> <code>dropout</code> <code>encoder</code> <code>heads</code> <code>hidden_dim</code> <code>in_size</code> <code>mlp_dim</code> <code>norm_layer</code> <code>out_size</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>patch_size</code> <code>representation_size</code> <code>seq_length</code>"},{"location":"api/models/#inferno.models.VisionTransformer.attention_dropout","title":"attention_dropout","text":"<pre><code>attention_dropout = attention_dropout\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.class_token","title":"class_token","text":"<pre><code>class_token = Parameter(zeros(1, 1, hidden_dim))\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.conv_proj","title":"conv_proj","text":"<pre><code>conv_proj = Conv2d(\n    in_channels=3,\n    out_channels=hidden_dim,\n    kernel_size=patch_size,\n    stride=patch_size,\n    cov=cov[\"conv_proj\"],\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.dropout","title":"dropout","text":"<pre><code>dropout = dropout\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.encoder","title":"encoder","text":"<pre><code>encoder = Encoder(\n    seq_length,\n    num_layers,\n    num_heads,\n    hidden_dim,\n    mlp_dim,\n    dropout,\n    attention_dropout,\n    norm_layer,\n    parametrization=parametrization,\n    cov=cov[\"encoder\"],\n)\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.heads","title":"heads","text":"<pre><code>heads = Sequential(heads_layers)\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.hidden_dim","title":"hidden_dim","text":"<pre><code>hidden_dim = hidden_dim\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.in_size","title":"in_size","text":"<pre><code>in_size = in_size\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.mlp_dim","title":"mlp_dim","text":"<pre><code>mlp_dim = mlp_dim\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.norm_layer","title":"norm_layer","text":"<pre><code>norm_layer = norm_layer\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.out_size","title":"out_size","text":"<pre><code>out_size = out_size\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.VisionTransformer.patch_size","title":"patch_size","text":"<pre><code>patch_size = patch_size\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.representation_size","title":"representation_size","text":"<pre><code>representation_size = representation_size\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.seq_length","title":"seq_length","text":"<pre><code>seq_length = seq_length\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.forward","title":"forward","text":"<pre><code>forward(\n    x: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.VisionTransformer.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    in_size: int,\n    out_size: int,\n    weights: Weights,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre> <p>Load a VisionTransformer model with pretrained weights.</p> <p>Depending on the <code>in_size</code> and <code>out_size</code> parameters, the first and last layers of the model are not initialized with the pretrained weights.</p> <p>Parameters:</p> Name Type Description Default <code>in_size</code> <code>int</code> <p>Size of the input (i.e. image size).</p> required <code>out_size</code> <code>int</code> <p>Size of the output (i.e. number of classes).</p> required <code>weights</code> <code>Weights</code> <p>Pretrained weights to use.</p> required <code>freeze</code> <code>bool</code> <p>Whether to freeze the pretrained weights.</p> <code>False</code>"},{"location":"api/models/#inferno.models.VisionTransformer.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Needs to be implemented because VisionTransformer has direct parameters.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.VisionTransformer.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>Needs to be implemented because VisionTransformer has direct parameters.</p>"},{"location":"api/models/#inferno.models.WideResNet101","title":"WideResNet101","text":"<pre><code>WideResNet101(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>WideResNet-101-2</p> <p>Architecture described in Wide Residual Networks. The model is the same as a ResNet except for the bottleneck number of channels which is twice larger in every block.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to <code>ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.WideResNet101.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=deepcopy(cov),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=deepcopy(cov),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.WideResNet101.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet101.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.WideResNet101.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/models/#inferno.models.WideResNet50","title":"WideResNet50","text":"<pre><code>WideResNet50(*args, **kwargs)\n</code></pre> <p>               Bases: <code>ResNet</code></p> <p>WideResNet-50-2</p> <p>Architecture described in Wide Residual Networks. The model is the same as a ResNet except for the bottleneck number of channels which is twice larger in every block.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed on to <code>ResNet</code>.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <code>from_pretrained_weights</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>avgpool</code> <code>base_width</code> <code>bn1</code> <code>conv1</code> <code>dilation</code> <code>fc</code> <code>groups</code> <code>inplanes</code> <code>layer1</code> <code>layer2</code> <code>layer3</code> <code>layer4</code> <code>optional_pool</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>relu</code>"},{"location":"api/models/#inferno.models.WideResNet50.avgpool","title":"avgpool","text":"<pre><code>avgpool = AdaptiveAvgPool2d((1, 1))\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.base_width","title":"base_width","text":"<pre><code>base_width = width_per_group\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.bn1","title":"bn1","text":"<pre><code>bn1 = norm_layer(inplanes)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.conv1","title":"conv1","text":"<pre><code>conv1 = Conv2d(\n    3,\n    inplanes,\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    bias=False,\n    cov=deepcopy(cov),\n    parametrization=parametrization,\n    layer_type=\"input\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.dilation","title":"dilation","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.fc","title":"fc","text":"<pre><code>fc = Linear(\n    512 * expansion,\n    out_size,\n    parametrization=parametrization,\n    cov=deepcopy(cov),\n    layer_type=\"output\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.inplanes","title":"inplanes","text":"<pre><code>inplanes = 64\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.layer1","title":"layer1","text":"<pre><code>layer1 = _make_layer(\n    block,\n    64,\n    num_blocks_per_layer[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.layer2","title":"layer2","text":"<pre><code>layer2 = _make_layer(\n    block,\n    128,\n    num_blocks_per_layer[1],\n    stride=2,\n    dilate=replace_stride_with_dilation[0],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.layer3","title":"layer3","text":"<pre><code>layer3 = _make_layer(\n    block,\n    256,\n    num_blocks_per_layer[2],\n    stride=2,\n    dilate=replace_stride_with_dilation[1],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.layer4","title":"layer4","text":"<pre><code>layer4 = _make_layer(\n    block,\n    512,\n    num_blocks_per_layer[3],\n    stride=2,\n    dilate=replace_stride_with_dilation[2],\n    parametrization=parametrization,\n    cov=(\n        deepcopy(cov)\n        if isinstance(cov, DiagonalCovariance)\n        else None\n    ),\n    layer_type=\"hidden\",\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.optional_pool","title":"optional_pool","text":"<pre><code>optional_pool = MaxPool2d(\n    kernel_size=3, stride=2, padding=1\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/models/#inferno.models.WideResNet50.relu","title":"relu","text":"<pre><code>relu = ReLU(inplace=True)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.from_pretrained_weights","title":"from_pretrained_weights","text":"<pre><code>from_pretrained_weights(\n    out_size: int,\n    architecture: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n    weights: Weights = DEFAULT,\n    freeze: bool = False,\n    *args,\n    **kwargs\n)\n</code></pre>"},{"location":"api/models/#inferno.models.WideResNet50.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/models/#inferno.models.WideResNet50.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/bnn/modules/","title":"Modules","text":""},{"location":"api/bnn/modules/#inferno.bnn.modules","title":"modules","text":"<p>Classes:</p> Name Description <code>BNNMixin</code> <p>Abstract mixin class turning a torch module into a Bayesian neural network module.</p> <code>Conv1d</code> <p>Applies a 1D convolution over an input signal composed of several input planes.</p> <code>Conv2d</code> <p>Applies a 2D convolution over an input signal composed of several input planes.</p> <code>Conv3d</code> <p>Applies a 3D convolution over an input signal composed of several input planes.</p> <code>Linear</code> <p>Applies an affine transformation to the input.</p> <code>MultiheadAttention</code> <p>Attention layer (with multiple heads).</p> <code>Sequential</code> <p>A sequential container for modules.</p> <code>SinusoidalPositionalEncoding</code> <p>Sinusoidal Positional Encoding.</p> <p>Functions:</p> Name Description <code>batched_forward</code> <p>Call a torch.nn.Module on inputs with arbitrary many batch dimensions rather than</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.BNNMixin","title":"BNNMixin","text":"<pre><code>BNNMixin(\n    parametrization: (\n        Parametrization | None\n    ) = MaximalUpdate(),\n    *args,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract mixin class turning a torch module into a Bayesian neural network module.</p> <p>Parameters:</p> Name Type Description Default <code>parametrization</code> <code>Parametrization | None</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass of the module.</p> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.BNNMixin.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.BNNMixin.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch *out_feature\"]\n</code></pre> <p>Forward pass of the module.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Float[Tensor, '*sample batch *in_feature']</code> <p>Input tensor.</p> required <code>sample_shape</code> <code>Size | None</code> <p>Shape of samples. If None, runs a forward pass with just the mean parameters.</p> <code>Size([])</code> <code>generator</code> <code>Generator | None</code> <p>Random number generator.</p> <code>None</code> <code>input_contains_samples</code> <code>bool</code> <p>Whether the input already contains samples. If True, the input is assumed to have <code>len(sample_shape)</code> many leading dimensions containing input samples (typically outputs from previous layers).</p> <code>False</code> <code>parameter_samples</code> <code>dict[str, Float[Tensor, '*sample parameter']] | None</code> <p>Dictionary of parameter samples. Used to pass sampled parameters to the module. Useful to jointly sample parameters of multiple layers.</p> <code>None</code>"},{"location":"api/bnn/modules/#inferno.bnn.modules.BNNMixin.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/bnn/modules/#inferno.bnn.modules.BNNMixin.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d","title":"Conv1d","text":"<pre><code>Conv1d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_1_t,\n    stride: _size_1_t = 1,\n    padding: str | _size_1_t = 0,\n    dilation: _size_1_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"zeros\",\n    layer_type: Literal[\n        \"input\", \"hidden\", \"output\"\n    ] = \"hidden\",\n    cov: FactorizedCovariance | None = None,\n    parametrization: Parametrization = MaximalUpdate(),\n    device: device | None = None,\n    dtype: dtype | None = None,\n)\n</code></pre> <p>               Bases: <code>_ConvNd</code></p> <p>Applies a 1D convolution over an input signal composed of several input planes.</p> <p>In the simplest case, the output value of the layer with input size :math:<code>(N, C_{\\text{in}}, L)</code> and output :math:<code>(N, C_{\\text{out}}, L_{\\text{out}})</code> can be precisely described as:</p> <p>.. math::     \\text{out}(N_i, C_{\\text{out}j}) = \\text{bias}(Cj}) +     \\sum, k)     \\star \\text{input}(N_i, k)}^{C_{in} - 1} \\text{weight}(C_{\\text{out}_j</p> <p>where :math:<code>\\star</code> is the valid <code>cross-correlation</code>_ operator, :math:<code>N</code> is a batch size, :math:<code>C</code> denotes a number of channels, :math:<code>L</code> is a length of signal sequence.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input image.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution.</p> required <code>kernel_size</code> <code>_size_1_t</code> <p>Size of the convolving kernel.</p> required <code>stride</code> <code>_size_1_t</code> <p>Stride of the convolution.</p> <code>1</code> <code>padding</code> <code>str | _size_1_t</code> <p>Padding added to both sides of the input.</p> <code>0</code> <code>dilation</code> <code>_size_1_t</code> <p>Spacing between kernel elements.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>If <code>True</code>, adds a learnable bias to the output.</p> <code>True</code> <code>padding_mode</code> <code>str</code> <p><code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default is <code>'zeros'</code>.</p> <code>'zeros'</code> <code>layer_type</code> <code>Literal['input', 'hidden', 'output']</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\". Controls the initialization and learning rate scaling of the parameters.</p> <code>'hidden'</code> <code>cov</code> <code>FactorizedCovariance | None</code> <p>The covariance of the parameters.</p> <code>None</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>device</code> <code>device | None</code> <p>The device on which to place the tensor.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>The desired data type of the returned tensor.</p> <code>None</code> <p>Methods:</p> Name Description <code>extra_repr</code> <code>forward</code> <code>parameters_and_lrs</code> <code>reset_parameters</code> <p>Reset the parameters of the module.</p> <p>Attributes:</p> Name Type Description <code>bias</code> <code>Parameter</code> <code>dilation</code> <code>groups</code> <code>in_channels</code> <code>kernel_size</code> <code>layer_type</code> <code>out_channels</code> <code>output_padding</code> <code>padding</code> <code>padding_mode</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>params</code> <code>stride</code> <code>transposed</code> <code>weight</code> <code>Parameter</code>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.bias","title":"bias","text":"<pre><code>bias: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.dilation","title":"dilation","text":"<pre><code>dilation = dilation\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.in_channels","title":"in_channels","text":"<pre><code>in_channels = in_channels\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.kernel_size","title":"kernel_size","text":"<pre><code>kernel_size = kernel_size\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.layer_type","title":"layer_type","text":"<pre><code>layer_type = layer_type\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.out_channels","title":"out_channels","text":"<pre><code>out_channels = out_channels\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.output_padding","title":"output_padding","text":"<pre><code>output_padding = output_padding\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.padding","title":"padding","text":"<pre><code>padding = padding\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.padding_mode","title":"padding_mode","text":"<pre><code>padding_mode = padding_mode\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.params","title":"params","text":"<pre><code>params = ParameterDict(mean_param_dict)\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.stride","title":"stride","text":"<pre><code>stride = stride\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.transposed","title":"transposed","text":"<pre><code>transposed = transposed\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.weight","title":"weight","text":"<pre><code>weight: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[\n    Tensor, \"*sample *batch out_channel *out_feature\"\n]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"] = \"SGD\"\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv1d.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d","title":"Conv2d","text":"<pre><code>Conv2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: str | _size_2_t = 0,\n    dilation: _size_2_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"zeros\",\n    layer_type: Literal[\n        \"input\", \"hidden\", \"output\"\n    ] = \"hidden\",\n    cov: FactorizedCovariance | None = None,\n    parametrization: Parametrization = MaximalUpdate(),\n    device: device | None = None,\n    dtype: dtype | None = None,\n)\n</code></pre> <p>               Bases: <code>_ConvNd</code></p> <p>Applies a 2D convolution over an input signal composed of several input planes.</p> <p>In the simplest case, the output value of the layer with input size :math:<code>(N, C_{\\text{in}}, H, W)</code> and output :math:<code>(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})</code> can be precisely described as:</p> <p>.. math::     \\text{out}(N_i, C_{\\text{out}j}) = \\text{bias}(Cj}) +     \\sum(N_i, k)}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input</p> <p>where :math:<code>\\star</code> is the valid 2D <code>cross-correlation</code>_ operator, :math:<code>N</code> is a batch size, :math:<code>C</code> denotes a number of channels, :math:<code>H</code> is a height of input planes in pixels, and :math:<code>W</code> is width in pixels.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input image.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution.</p> required <code>kernel_size</code> <code>_size_2_t</code> <p>Size of the convolving kernel.</p> required <code>stride</code> <code>_size_2_t</code> <p>Stride of the convolution.</p> <code>1</code> <code>padding</code> <code>str | _size_2_t</code> <p>Padding added to all four sides of the input.</p> <code>0</code> <code>dilation</code> <code>_size_2_t</code> <p>Spacing between kernel elements.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>If <code>True</code>, adds a learnable bias to the output.</p> <code>True</code> <code>padding_mode</code> <code>str</code> <p><code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default is <code>'zeros'</code>.</p> <code>'zeros'</code> <code>layer_type</code> <code>Literal['input', 'hidden', 'output']</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\". Controls the initialization and learning rate scaling of the parameters.</p> <code>'hidden'</code> <code>cov</code> <code>FactorizedCovariance | None</code> <p>The covariance of the parameters.</p> <code>None</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>device</code> <code>device | None</code> <p>The device on which to place the tensor.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>The desired data type of the returned tensor.</p> <code>None</code> <p>Methods:</p> Name Description <code>extra_repr</code> <code>forward</code> <code>parameters_and_lrs</code> <code>reset_parameters</code> <p>Reset the parameters of the module.</p> <p>Attributes:</p> Name Type Description <code>bias</code> <code>Parameter</code> <code>dilation</code> <code>groups</code> <code>in_channels</code> <code>kernel_size</code> <code>layer_type</code> <code>out_channels</code> <code>output_padding</code> <code>padding</code> <code>padding_mode</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>params</code> <code>stride</code> <code>transposed</code> <code>weight</code> <code>Parameter</code>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.bias","title":"bias","text":"<pre><code>bias: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.dilation","title":"dilation","text":"<pre><code>dilation = dilation\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.in_channels","title":"in_channels","text":"<pre><code>in_channels = in_channels\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.kernel_size","title":"kernel_size","text":"<pre><code>kernel_size = kernel_size\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.layer_type","title":"layer_type","text":"<pre><code>layer_type = layer_type\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.out_channels","title":"out_channels","text":"<pre><code>out_channels = out_channels\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.output_padding","title":"output_padding","text":"<pre><code>output_padding = output_padding\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.padding","title":"padding","text":"<pre><code>padding = padding\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.padding_mode","title":"padding_mode","text":"<pre><code>padding_mode = padding_mode\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.params","title":"params","text":"<pre><code>params = ParameterDict(mean_param_dict)\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.stride","title":"stride","text":"<pre><code>stride = stride\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.transposed","title":"transposed","text":"<pre><code>transposed = transposed\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.weight","title":"weight","text":"<pre><code>weight: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[\n    Tensor, \"*sample *batch out_channel *out_feature\"\n]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"] = \"SGD\"\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv2d.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d","title":"Conv3d","text":"<pre><code>Conv3d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_3_t,\n    stride: _size_3_t = 1,\n    padding: str | _size_3_t = 0,\n    dilation: _size_3_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"zeros\",\n    layer_type: Literal[\n        \"input\", \"hidden\", \"output\"\n    ] = \"hidden\",\n    cov: FactorizedCovariance | None = None,\n    parametrization: Parametrization = MaximalUpdate(),\n    device: device | None = None,\n    dtype: dtype | None = None,\n)\n</code></pre> <p>               Bases: <code>_ConvNd</code></p> <p>Applies a 3D convolution over an input signal composed of several input planes.</p> <p>In the simplest case, the output value of the layer with input size :math:<code>(N, C_{in}, D, H, W)</code> and output :math:<code>(N, C_{out}, D_{out}, H_{out}, W_{out})</code> can be precisely described as:</p> <p>.. math::     out(N_i, C_{out_j}) = bias(C_{out_j}) +                             \\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \\star input(N_i, k)</p> <p>where :math:<code>\\star</code> is the valid 3D <code>cross-correlation</code>_ operator</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input image.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution.</p> required <code>kernel_size</code> <code>_size_3_t</code> <p>Size of the convolving kernel.</p> required <code>stride</code> <code>_size_3_t</code> <p>Stride of the convolution.</p> <code>1</code> <code>padding</code> <code>str | _size_3_t</code> <p>Padding added to all six sides of the input.</p> <code>0</code> <code>dilation</code> <code>_size_3_t</code> <p>Spacing between kernel elements.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>If <code>True</code>, adds a learnable bias to the output.</p> <code>True</code> <code>padding_mode</code> <code>str</code> <p><code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default is <code>'zeros'</code>.</p> <code>'zeros'</code> <code>layer_type</code> <code>Literal['input', 'hidden', 'output']</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\". Controls the initialization and learning rate scaling of the parameters.</p> <code>'hidden'</code> <code>cov</code> <code>FactorizedCovariance | None</code> <p>The covariance of the parameters.</p> <code>None</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>device</code> <code>device | None</code> <p>The device on which to place the tensor.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>The desired data type of the returned tensor.</p> <code>None</code> <p>Methods:</p> Name Description <code>extra_repr</code> <code>forward</code> <code>parameters_and_lrs</code> <code>reset_parameters</code> <p>Reset the parameters of the module.</p> <p>Attributes:</p> Name Type Description <code>bias</code> <code>Parameter</code> <code>dilation</code> <code>groups</code> <code>in_channels</code> <code>kernel_size</code> <code>layer_type</code> <code>out_channels</code> <code>output_padding</code> <code>padding</code> <code>padding_mode</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>params</code> <code>stride</code> <code>transposed</code> <code>weight</code> <code>Parameter</code>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.bias","title":"bias","text":"<pre><code>bias: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.dilation","title":"dilation","text":"<pre><code>dilation = dilation\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.groups","title":"groups","text":"<pre><code>groups = groups\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.in_channels","title":"in_channels","text":"<pre><code>in_channels = in_channels\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.kernel_size","title":"kernel_size","text":"<pre><code>kernel_size = kernel_size\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.layer_type","title":"layer_type","text":"<pre><code>layer_type = layer_type\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.out_channels","title":"out_channels","text":"<pre><code>out_channels = out_channels\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.output_padding","title":"output_padding","text":"<pre><code>output_padding = output_padding\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.padding","title":"padding","text":"<pre><code>padding = padding\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.padding_mode","title":"padding_mode","text":"<pre><code>padding_mode = padding_mode\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.params","title":"params","text":"<pre><code>params = ParameterDict(mean_param_dict)\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.stride","title":"stride","text":"<pre><code>stride = stride\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.transposed","title":"transposed","text":"<pre><code>transposed = transposed\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.weight","title":"weight","text":"<pre><code>weight: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample batch *in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[\n    Tensor, \"*sample *batch out_channel *out_feature\"\n]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"] = \"SGD\"\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Conv3d.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear","title":"Linear","text":"<pre><code>Linear(\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    layer_type: Literal[\n        \"input\", \"hidden\", \"output\"\n    ] = \"hidden\",\n    cov: FactorizedCovariance | None = None,\n    parametrization: Parametrization = MaximalUpdate(),\n    device: device | None = None,\n    dtype: dtype | None = None,\n)\n</code></pre> <p>               Bases: <code>BNNMixin</code>, <code>Module</code></p> <p>Applies an affine transformation to the input.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Size of each input sample.</p> required <code>out_features</code> <code>int</code> <p>Size of each output sample.</p> required <code>bias</code> <code>bool</code> <p>If set to <code>False</code>, the layer will not learn an additive bias.</p> <code>True</code> <code>layer_type</code> <code>Literal['input', 'hidden', 'output']</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\". Controls the initialization and learning rate scaling of the parameters.</p> <code>'hidden'</code> <code>cov</code> <code>FactorizedCovariance | None</code> <p>Covariance object for the parameters.</p> <code>None</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>device</code> <code>device | None</code> <p>Device on which to instantiate the parameters.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Data type of the parameters.</p> <code>None</code> <p>Methods:</p> Name Description <code>extra_repr</code> <code>forward</code> <code>parameters_and_lrs</code> <code>reset_parameters</code> <p>Reset the parameters of the module.</p> <p>Attributes:</p> Name Type Description <code>bias</code> <code>Parameter</code> <code>in_features</code> <code>layer_type</code> <code>out_features</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>params</code> <code>weight</code> <code>Parameter</code>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.bias","title":"bias","text":"<pre><code>bias: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.in_features","title":"in_features","text":"<pre><code>in_features = in_features\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.layer_type","title":"layer_type","text":"<pre><code>layer_type = layer_type\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.out_features","title":"out_features","text":"<pre><code>out_features = out_features\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.params","title":"params","text":"<pre><code>params = ParameterDict(mean_param_dict)\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.weight","title":"weight","text":"<pre><code>weight: Parameter\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr() -&gt; str\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*sample *batch in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch out_feature\"]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"] = \"SGD\"\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Linear.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention","title":"MultiheadAttention","text":"<pre><code>MultiheadAttention(\n    embed_dim: int,\n    num_heads: int,\n    dropout: float = 0.0,\n    bias: bool = True,\n    kdim: int | None = None,\n    vdim: int | None = None,\n    embed_dim_out: int | None = None,\n    out_proj: bool = True,\n    cov: (\n        FactorizedCovariance\n        | dict[FactorizedCovariance]\n        | None\n    ) = None,\n    parametrization: Parametrization = MaximalUpdate(),\n    device: device | None = None,\n    dtype: dtype | None = None,\n)\n</code></pre> <p>               Bases: <code>BNNMixin</code>, <code>Module</code></p> <p>Attention layer (with multiple heads).</p> <p>Multi-head (self-)attention layer with an optional attention mask, allowing a model to jointly attend to information from different representation subspaces. Consists of <code>num_heads</code> scaled dot-product attention modules, whose outputs are concatenated and then combined into an output sequence via a linear layer.</p> <p>The module supports nested or padded tensors and is inspired by the following implementation.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Dimensionality of the inputs and outputs to the layer (i.e. dimensionality of the query embeddings).</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout</code> <code>float</code> <p>Dropout probability; if greater than 0.0, dropout is applied.</p> <code>0.0</code> <code>bias</code> <code>bool</code> <p>Whether to add bias to query, key, value and output projections.</p> <code>True</code> <code>kdim</code> <code>int | None</code> <p>Dimensionality of the key embeddings.</p> <code>None</code> <code>vdim</code> <code>int | None</code> <p>Dimensionality of the value embeddings.</p> <code>None</code> <code>embed_dim_out</code> <code>int | None</code> <p>Dimensionality of the output embeddings. If <code>None</code>, set to <code>embed_dim</code>.</p> <code>None</code> <code>out_proj</code> <code>bool</code> <p>Whether to include the output projection layer.</p> <code>True</code> <code>cov</code> <code>FactorizedCovariance | dict[FactorizedCovariance] | None</code> <p>Covariance structure of the weights. Either a single covariance structure used in all linear projections, or a dictionary with keys <code>k</code>, <code>q</code>, <code>v</code> and <code>out</code> and values containing either covariance structures or <code>None</code>.</p> <code>None</code> <code>parametrization</code> <code>Parametrization</code> <p>The parametrization to use. Defines the initialization and learning rate scaling for the parameters of the module.</p> <code>MaximalUpdate()</code> <code>device</code> <code>device | None</code> <p>Device on which to instantiate the parameters.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Data type of the parameters.</p> <code>None</code> <p>Methods:</p> Name Description <code>forward</code> <p>Computes scaled dot product attention on query, key and value tensors, using an optional attention mask.</p> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>bias</code> <code>dropout</code> <code>embed_dim</code> <code>embed_dim_out</code> <code>head_dim</code> <code>k_proj</code> <code>kdim</code> <code>num_heads</code> <code>out_proj</code> <code>parametrization</code> <code>Parametrization</code> <p>Parametrization of the module.</p> <code>q_proj</code> <code>v_proj</code> <code>vdim</code>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.bias","title":"bias","text":"<pre><code>bias = bias\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.dropout","title":"dropout","text":"<pre><code>dropout = dropout\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.embed_dim","title":"embed_dim","text":"<pre><code>embed_dim = embed_dim\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.embed_dim_out","title":"embed_dim_out","text":"<pre><code>embed_dim_out = (\n    embed_dim_out\n    if embed_dim_out is not None\n    else embed_dim\n)\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.head_dim","title":"head_dim","text":"<pre><code>head_dim = embed_dim // num_heads\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.k_proj","title":"k_proj","text":"<pre><code>k_proj = Linear(\n    kdim,\n    embed_dim,\n    bias=bias,\n    cov=cov[\"k\"],\n    parametrization=parametrization,\n    **factory_kwargs\n)\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.kdim","title":"kdim","text":"<pre><code>kdim = kdim if kdim is not None else embed_dim\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.num_heads","title":"num_heads","text":"<pre><code>num_heads = num_heads\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.out_proj","title":"out_proj","text":"<pre><code>out_proj = (\n    Linear(\n        embed_dim,\n        embed_dim_out,\n        bias=bias,\n        cov=cov[\"out\"],\n        parametrization=parametrization,\n        **factory_kwargs\n    )\n    if out_proj\n    else None\n)\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.parametrization","title":"parametrization","text":"<pre><code>parametrization: Parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.q_proj","title":"q_proj","text":"<pre><code>q_proj = Linear(\n    embed_dim,\n    embed_dim,\n    bias=bias,\n    cov=cov[\"q\"],\n    parametrization=parametrization,\n    **factory_kwargs\n)\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.v_proj","title":"v_proj","text":"<pre><code>v_proj = Linear(\n    vdim,\n    embed_dim,\n    bias=bias,\n    cov=cov[\"v\"],\n    parametrization=parametrization,\n    **factory_kwargs\n)\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.vdim","title":"vdim","text":"<pre><code>vdim = vdim if vdim is not None else embed_dim\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.forward","title":"forward","text":"<pre><code>forward(\n    query: Float[\n        Tensor, \"*sample batch query_token embed_dim\"\n    ],\n    key: (\n        Float[\n            Tensor, \"*sample batch keyval_token embed_dim_k\"\n        ]\n        | None\n    ),\n    value: (\n        Float[Tensor, \"*sample batch token embed_dim_v\"]\n        | None\n    ),\n    attn_mask: (\n        Float[Tensor, \"batch query_token keyval_token\"]\n        | None\n    ) = None,\n    is_causal: bool = False,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample batch query_token embed_dim\"]\n</code></pre> <p>Computes scaled dot product attention on query, key and value tensors, using an optional attention mask.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Float[Tensor, '*sample batch query_token embed_dim']</code> <p>Query tensor / embeddings.</p> required <code>key</code> <code>Float[Tensor, '*sample batch keyval_token embed_dim_k'] | None</code> <p>Key tensor / embeddings.</p> required <code>value</code> <code>Float[Tensor, '*sample batch token embed_dim_v'] | None</code> <p>Value tensor / embeddings.</p> required <code>attn_mask</code> <code>Float[Tensor, 'batch query_token keyval_token'] | None</code> <p>Attention mask; shape must be broadcastable to the shape of attention weights. Two types of masks are supported. A boolean mask where a value of True indicates that the element should take part in attention. A float mask of the same type as query, key, value that is added to the attention score.</p> <code>None</code> <code>is_causal</code> <code>bool</code> <p>If set to true, the attention masking is a lower triangular matrix when the mask is a square matrix. The attention masking has the form of the upper left causal bias due to the alignment (see <code>torch.nn.attention.bias.CausalBias</code>) when the mask is a non-square matrix. An error is thrown if both <code>attn_mask</code> and <code>is_causal</code> are set.</p> <code>False</code> <code>sample_shape</code> <code>Size | None</code> <p>Shape of samples. If None, runs a forward pass with just the mean parameters.</p> <code>Size([])</code> <code>generator</code> <code>Generator | None</code> <p>Random number generator.</p> <code>None</code> <code>input_contains_samples</code> <code>bool</code> <p>Whether the input already contains samples. If True, the input is assumed to have <code>len(sample_shape)</code> many leading dimensions containing input samples (typically outputs from previous layers).</p> <code>False</code> <code>parameter_samples</code> <code>dict[str, Float[Tensor, '*sample parameter']] | None</code> <p>Dictionary of parameter samples. Used to pass sampled parameters to the module. Useful to jointly sample parameters of multiple layers.</p> <code>None</code>"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/bnn/modules/#inferno.bnn.modules.MultiheadAttention.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Sequential","title":"Sequential","text":"<pre><code>Sequential(\n    *args: BNNMixin | Module,\n    parametrization: Parametrization | None = None\n)\n</code></pre><pre><code>Sequential(\n    arg: OrderedDict[str, BNNMixin | Module],\n    parametrization: Parametrization | None = None,\n)\n</code></pre> <pre><code>Sequential(\n    *args, parametrization: Parametrization | None = None\n)\n</code></pre> <p>               Bases: <code>BNNMixin</code>, <code>Sequential</code></p> <p>A sequential container for modules.</p> <p>Modules will be added to it in the order they are passed in the constructor. Alternatively, an <code>OrderedDict</code> of modules can be passed in. The <code>forward()</code> method of <code>Sequential</code> accepts any input and forwards it to the first module it contains. It then \"chains\" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.</p> <p>The value a <code>Sequential</code> provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the <code>Sequential</code> applies to each of the modules it stores (which are each a registered submodule of the <code>Sequential</code>)</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Any number of modules to add to the container.</p> <code>()</code> <code>parametrization</code> <code>Parametrization | None</code> <p>The parametrization to use. If <code>None</code>, the parametrization of the modules in the container will be used. If a <code>Parametrization</code> object is passed, it will be used for all modules in the container.</p> <code>None</code> <p>Methods:</p> Name Description <code>forward</code> <code>parameters_and_lrs</code> <p>Get the parameters of the module and their learning rates for the chosen optimizer</p> <code>reset_parameters</code> <p>Reset the parameters of the module and set the parametrization of all children</p> <p>Attributes:</p> Name Type Description <code>parametrization</code> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Sequential.parametrization","title":"parametrization","text":"<pre><code>parametrization = parametrization\n</code></pre> <p>Parametrization of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Sequential.forward","title":"forward","text":"<pre><code>forward(\n    input: Float[Tensor, \"*batch in_feature\"],\n    /,\n    sample_shape: Size | None = Size([]),\n    generator: Generator | None = None,\n    input_contains_samples: bool = False,\n    parameter_samples: (\n        dict[str, Float[Tensor, \"*sample parameter\"]] | None\n    ) = None,\n) -&gt; Float[Tensor, \"*sample *batch out_feature\"]\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.Sequential.parameters_and_lrs","title":"parameters_and_lrs","text":"<pre><code>parameters_and_lrs(\n    lr: float, optimizer: Literal[\"SGD\", \"Adam\"]\n) -&gt; list[dict[str, Tensor | float]]\n</code></pre> <p>Get the parameters of the module and their learning rates for the chosen optimizer and the parametrization of the module.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>The global learning rate.</p> required <code>optimizer</code> <code>Literal['SGD', 'Adam']</code> <p>The optimizer being used.</p> required"},{"location":"api/bnn/modules/#inferno.bnn.modules.Sequential.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters of the module and set the parametrization of all children to the parametrization of the module.</p> <p>This method should be implemented by subclasses to reset the parameters of the module.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.SinusoidalPositionalEncoding","title":"SinusoidalPositionalEncoding","text":"<pre><code>SinusoidalPositionalEncoding(\n    embed_dim: int,\n    max_seq_len: int = 4096,\n    base: int = 10000,\n    device: device | None = None,\n    dtype: dtype | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Sinusoidal Positional Encoding.</p> <p>This module adds fixed sinusoidal positional embeddings (Vaswani et al., 2017; Sec. 3.5)  to input embeddings to provide the model with information about the relative or absolute position of tokens in a sequence.</p> <p>The sinusoidal positional encoding uses sine and cosine functions of different frequencies:</p> \\[ \\begin{align*}     \\operatorname{PE}(\\text{pos}, 2i)   &amp;= \\sin(\\text{base}^{-\\frac{2i}{\\text{embed\\_dim}}} \\cdot \\text{pos}) \\\\     \\operatorname{PE}(\\text{pos}, 2i+1) &amp;= \\cos(\\text{base}^{-\\frac{2i}{\\text{embed\\_dim}}} \\cdot \\text{pos}) \\end{align*} \\] <p>where \\(\\text{pos}\\) is the position index, \\(0\\leq i \\leq \\frac{\\text{embed\\_dim}{2}\\) is the embedding dimension  index and \\(\\text{embed\\_dim}\\) is the embedding dimensionality.</p> <p>The encoding is designed so that each dimension of the positional encoding corresponds to a sinusoid with wavelengths forming a geometric progression from 2\u03c0 to base\u00b72\u03c0. This allows the model to easily learn to attend by relative positions.</p> <p>Notes:</p> <ul> <li>The positional encodings are added to the input embeddings, so both must have     the same embedding dimension (<code>embed_dim</code>).</li> <li>If the input sequence length exceeds <code>max_seq_len</code>, the encoding will be truncated     to match the input length, which may cause issues. Ensure <code>max_seq_len</code> \u2265 expected     sequence lengths.</li> <li>The encoding is deterministic and does not require training.</li> </ul> <pre><code>import torch\nfrom inferno.bnn.modules import SinusoidalPositionalEncoding\n\n# Create positional encoding for 512-dim embeddings\npos_enc = SinusoidalPositionalEncoding(embed_dim=512)\n\n# Apply to input embeddings\nbatch_size, seq_len, embed_dim = 32, 100, 512\ninput_embeddings = torch.randn(batch_size, seq_len, embed_dim)\noutput = pos_enc(input_embeddings)\nprint(output.shape)  # torch.Size([32, 100, 512])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The embedding dimension. Should be even for proper sine/cosine pairing.</p> required <code>max_seq_len</code> <code>int</code> <p>Maximum sequence length to precompute encodings for.</p> <code>4096</code> <code>base</code> <code>int</code> <p>Base of the angular frequency.</p> <code>10000</code> <code>dtype</code> <code>dtype | None</code> <p>Data type for the positional encodings.</p> <code>None</code> <code>device</code> <code>device | None</code> <p>Device to place the positional encodings on.</p> <code>None</code> <p>Methods:</p> Name Description <code>forward</code> <p>Add positional encoding to input embeddings.</p> <p>Attributes:</p> Name Type Description <code>base</code> <code>max_seq_len</code>"},{"location":"api/bnn/modules/#inferno.bnn.modules.SinusoidalPositionalEncoding.base","title":"base","text":"<pre><code>base = base\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.SinusoidalPositionalEncoding.max_seq_len","title":"max_seq_len","text":"<pre><code>max_seq_len = max_seq_len\n</code></pre>"},{"location":"api/bnn/modules/#inferno.bnn.modules.SinusoidalPositionalEncoding.forward","title":"forward","text":"<pre><code>forward(\n    x: Float[Tensor, \"batch token embed_dim\"],\n) -&gt; Float[Tensor, \"batch token embed_dim\"]\n</code></pre> <p>Add positional encoding to input embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Tensor, 'batch token embed_dim']</code> <p>Input embeddings.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, 'batch token embed_dim']</code> <p>Input embeddings with added positional encoding of the same shape.</p>"},{"location":"api/bnn/modules/#inferno.bnn.modules.batched_forward","title":"batched_forward","text":"<pre><code>batched_forward(\n    obj: Module, num_batch_dims: int\n) -&gt; Callable[\n    [Float[Tensor, \"*sample batch *in_feature\"]],\n    Float[Tensor, \"*sample batch *out_feature\"],\n]\n</code></pre> <p>Call a torch.nn.Module on inputs with arbitrary many batch dimensions rather than just a single one.</p> <p>This is useful to extend the functionality of a torch.nn.Module to work with arbitrary many batch dimensions, for example arbitrary many sampling dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Module</code> <p>The torch.nn.Module to call.</p> required <code>num_batch_dims</code> <code>int</code> <p>The number of batch dimensions.</p> required"},{"location":"api/bnn/temperature_scaling/","title":"Temperature Scaling","text":""},{"location":"api/bnn/temperature_scaling/#inferno.bnn","title":"bnn","text":"<p>Basic building blocks for Bayesian neural networks.</p> <p>Classes:</p> Name Description <code>TemperatureScaler</code> <p>Temperature scaling.</p>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler","title":"TemperatureScaler","text":"<pre><code>TemperatureScaler(\n    loss_fn: Module = CrossEntropyLoss(),\n    lr: float = 0.1,\n    max_iter: int = 100,\n    tolerance_grad: float = 1e-07,\n    tolerance_change: float = 1e-09,\n    history_size: int = 100,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Temperature scaling.</p> <p>Tunes the temperature parameter of the model in the last layer to minimize the negative log likelihood of the validation set.</p> <p>Based on On Calibration of Modern Neural Networks.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>Module</code> <p>The loss function to be used for calibration.</p> <code>CrossEntropyLoss()</code> <code>lr</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>0.1</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations per optimization step.</p> <code>100</code> <code>tolerance_grad</code> <code>float</code> <p>Tolerance for the gradient.</p> <code>1e-07</code> <code>tolerance_change</code> <code>float</code> <p>Tolerance for the change in the loss function / parameters.</p> <code>1e-09</code> <code>history_size</code> <code>int</code> <p>Size of the history for the LBFGS optimizer.</p> <code>100</code> <p>Methods:</p> Name Description <code>optimize</code> <p>Optimizes the temperature of the model.</p> <p>Attributes:</p> Name Type Description <code>history_size</code> <code>loss_fn</code> <code>lr</code> <code>max_iter</code> <code>tolerance_change</code> <code>tolerance_grad</code>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler.history_size","title":"history_size","text":"<pre><code>history_size = history_size\n</code></pre>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler.loss_fn","title":"loss_fn","text":"<pre><code>loss_fn = loss_fn\n</code></pre>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler.lr","title":"lr","text":"<pre><code>lr = lr\n</code></pre>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler.max_iter","title":"max_iter","text":"<pre><code>max_iter = max_iter\n</code></pre>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler.tolerance_change","title":"tolerance_change","text":"<pre><code>tolerance_change = tolerance_change\n</code></pre>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler.tolerance_grad","title":"tolerance_grad","text":"<pre><code>tolerance_grad = tolerance_grad\n</code></pre>"},{"location":"api/bnn/temperature_scaling/#inferno.bnn.TemperatureScaler.optimize","title":"optimize","text":"<pre><code>optimize(model: Module, dataloader: DataLoader) -&gt; None\n</code></pre> <p>Optimizes the temperature of the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be calibrated, assumed to return logits.</p> required <code>dataloader</code> <code>DataLoader</code> <p>The dataloader for the dataset to calibrate on (typically the validation set).</p> required"},{"location":"api/bnn/bnn.params/parameters/","title":"Parameters","text":""},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params","title":"params","text":"<p>Classes:</p> Name Description <code>BNNParameter</code> <code>GaussianParameter</code> <p>Parameter of a BNN module with Gaussian distribution.</p> <code>FactorizedCovariance</code> <p>Covariance of a Gaussian parameter with a factorized structure.</p> <code>DiagonalCovariance</code> <p>Covariance of a Gaussian parameter with diagonal structure.</p> <code>KroneckerCovariance</code> <p>Covariance of a Gaussian parameter with Kronecker structure.</p> <code>LowRankCovariance</code> <p>Covariance of a Gaussian parameter with low-rank structure.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.BNNParameter","title":"BNNParameter","text":"<pre><code>BNNParameter(\n    hyperparameters: (\n        dict[str, Float[Tensor, \"*hyperparameter\"]] | None\n    ) = None,\n)\n</code></pre> <p>               Bases: <code>ParameterDict</code>, <code>ABC</code></p> <p>Methods:</p> Name Description <code>sample</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.BNNParameter.sample","title":"sample","text":"<pre><code>sample(\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n) -&gt; Float[Tensor, \"*sample parameter\"]\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.GaussianParameter","title":"GaussianParameter","text":"<pre><code>GaussianParameter(\n    mean: (\n        Float[Tensor, \"parameter\"]\n        | dict[str, Float[Tensor, \"parameter\"]]\n    ),\n    cov: FactorizedCovariance,\n)\n</code></pre> <p>               Bases: <code>BNNParameter</code></p> <p>Parameter of a BNN module with Gaussian distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Float[Tensor, 'parameter'] | dict[str, Float[Tensor, 'parameter']]</code> <p>Mean of the Gaussian distribution.</p> required <code>cov</code> <code>FactorizedCovariance</code> <p>Covariance of the Gaussian distribution.</p> required <p>Methods:</p> Name Description <code>sample</code> <p>Attributes:</p> Name Type Description <code>cov</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.GaussianParameter.cov","title":"cov","text":"<pre><code>cov = cov\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.GaussianParameter.sample","title":"sample","text":"<pre><code>sample(\n    sample_shape: Size = Size([]),\n    generator: Generator | None = None,\n) -&gt; (\n    Float[Tensor, \"*sample parameter\"]\n    | dict[str, Float[Tensor, \"*sample parameter\"]]\n)\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.FactorizedCovariance","title":"FactorizedCovariance","text":"<pre><code>FactorizedCovariance(rank: int | None = None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Covariance of a Gaussian parameter with a factorized structure.</p> <p>Assumes the covariance is factorized as a product of a square matrix and its transpose.</p> <p>..math::     \\mathbf{\\Sigma} = \\mathbf{S} \\mathbf{S}^\\top</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int | None</code> <p>Rank of the covariance matrix. If <code>None</code>, the rank is set to the total number of mean parameters.</p> <code>None</code> <p>Methods:</p> Name Description <code>factor_matmul</code> <p>Multiply left factor of the covariance matrix with the input.</p> <code>initialize_parameters</code> <p>Initialize the covariance parameters.</p> <code>reset_parameters</code> <p>Reset the parameters of the covariance matrix.</p> <code>to_dense</code> <p>Convert the covariance matrix to a dense representation.</p> <p>Attributes:</p> Name Type Description <code>lr_scaling</code> <code>dict[str, float]</code> <p>Compute the learning rate scaling for the covariance parameters.</p> <code>rank</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.FactorizedCovariance.lr_scaling","title":"lr_scaling","text":"<pre><code>lr_scaling: dict[str, float]\n</code></pre> <p>Compute the learning rate scaling for the covariance parameters.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.FactorizedCovariance.rank","title":"rank","text":"<pre><code>rank = rank\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.FactorizedCovariance.factor_matmul","title":"factor_matmul","text":"<pre><code>factor_matmul(\n    input: Float[Tensor, \"*sample parameter\"],\n    /,\n    additive_constant: (\n        Float[Tensor, \"*sample parameter\"] | None\n    ) = None,\n) -&gt; dict[str, Float[Tensor, \"*sample parameter\"]]\n</code></pre> <p>Multiply left factor of the covariance matrix with the input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Float[Tensor, '*sample parameter']</code> <p>Input tensor.</p> required <code>additive_constant</code> <code>Float[Tensor, '*sample parameter'] | None</code> <p>Additive constant to be added to the output.</p> <code>None</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.FactorizedCovariance.initialize_parameters","title":"initialize_parameters","text":"<pre><code>initialize_parameters(\n    mean_parameters: dict[str, Tensor],\n) -&gt; None\n</code></pre> <p>Initialize the covariance parameters.</p> <p>Parameters:</p> Name Type Description Default <code>mean_parameters</code> <code>dict[str, Tensor]</code> <p>Mean parameters of the Gaussian distribution.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Covariance parameters.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.FactorizedCovariance.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters(\n    mean_parameter_scales: dict[str, float] | float = 1.0,\n) -&gt; None\n</code></pre> <p>Reset the parameters of the covariance matrix.</p> <p>Initalizes the parameters of the covariance matrix with a scale that is given by the mean parameter scales and a covariance-specific scaling that depends on the structure of the covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>mean_parameter_scales</code> <code>dict[str, float] | float</code> <p>Scales of the mean parameters. If a dictionary keys are the names of the mean parameters. If a float, all covariance parameters are initialized with the same scale.</p> <code>1.0</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.FactorizedCovariance.to_dense","title":"to_dense","text":"<pre><code>to_dense() -&gt; Float[Tensor, 'parameter parameter']\n</code></pre> <p>Convert the covariance matrix to a dense representation.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.DiagonalCovariance","title":"DiagonalCovariance","text":"<pre><code>DiagonalCovariance()\n</code></pre> <p>               Bases: <code>FactorizedCovariance</code></p> <p>Covariance of a Gaussian parameter with diagonal structure.</p> <p>Methods:</p> Name Description <code>factor_matmul</code> <code>initialize_parameters</code> <code>reset_parameters</code> <code>to_dense</code> <p>Convert the covariance matrix to a dense representation.</p> <p>Attributes:</p> Name Type Description <code>lr_scaling</code> <code>dict[str, float]</code> <code>rank</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.DiagonalCovariance.lr_scaling","title":"lr_scaling","text":"<pre><code>lr_scaling: dict[str, float]\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.DiagonalCovariance.rank","title":"rank","text":"<pre><code>rank = rank\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.DiagonalCovariance.factor_matmul","title":"factor_matmul","text":"<pre><code>factor_matmul(\n    input: Float[Tensor, \"*sample parameter\"],\n    /,\n    additive_constant: (\n        Float[Tensor, \"*sample parameter\"] | None\n    ) = None,\n) -&gt; dict[str, Float[Tensor, \"*sample parameter\"]]\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.DiagonalCovariance.initialize_parameters","title":"initialize_parameters","text":"<pre><code>initialize_parameters(\n    mean_parameters: dict[str, Tensor],\n) -&gt; None\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.DiagonalCovariance.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters(\n    mean_parameter_scales: dict[str, float] | float = 1.0,\n) -&gt; None\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.DiagonalCovariance.to_dense","title":"to_dense","text":"<pre><code>to_dense() -&gt; Float[Tensor, 'parameter parameter']\n</code></pre> <p>Convert the covariance matrix to a dense representation.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance","title":"KroneckerCovariance","text":"<pre><code>KroneckerCovariance(\n    input_rank: int | None = None,\n    output_rank: int | None = None,\n)\n</code></pre> <p>               Bases: <code>FactorizedCovariance</code></p> <p>Covariance of a Gaussian parameter with Kronecker structure.</p> <p>Assumes the covariance is given by a Kronecker product of two matrices of size equal to the number of inputs and outputs to the layer. Each Kronecker factor is assumed to be of rank \\(R \\leq D\\) where \\(D\\) is either the input or output dimension of the layer.</p> <p>More precisely, the covariance is given by</p> \\[ \\begin{align*}     \\mathbf{\\Sigma} &amp;= \\mathbf{C}_{\\text{in}} \\otimes \\mathbf{C}_{\\text{out}}\\\\                     &amp;= \\mathbf{S}_{\\text{in}}\\mathbf{S}_{\\text{in}}^\\top \\otimes \\mathbf{S}_{\\text{out}}\\mathbf{S}_{\\text{out}}^\\top\\\\                     &amp;= (\\mathbf{S}_{\\text{in}} \\otimes \\mathbf{S}_{\\text{out}}) (\\mathbf{S}_{\\text{in}}^\\top \\otimes \\mathbf{S}_{\\text{out}}^\\top) \\end{align*} \\] <p>where \\(\\mathbf{S}_{\\text{in}}\\) and \\(\\mathbf{S}_{\\text{out}}\\) are the low-rank factors of the Kronecker factors \\(\\mathbf{C}_{\\text{in}}\\) and  \\(\\mathbf{C}_{\\text{out}}\\).</p> <p>Parameters:</p> Name Type Description Default <code>input_rank</code> <code>int | None</code> <p>Rank of the input Kronecker factor. If None, assumes full rank.</p> <code>None</code> <code>output_rank</code> <code>int | None</code> <p>Rank of the output Kronecker factor. If None, assumes full rank.</p> <code>None</code> <p>Methods:</p> Name Description <code>factor_matmul</code> <code>initialize_parameters</code> <code>reset_parameters</code> <code>to_dense</code> <p>Attributes:</p> Name Type Description <code>input_rank</code> <code>lr_scaling</code> <code>dict[str, float]</code> <p>Compute the learning rate scaling for the covariance parameters.</p> <code>output_rank</code> <code>rank</code> <code>sample_scale</code> <code>float</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.input_rank","title":"input_rank","text":"<pre><code>input_rank = input_rank\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.lr_scaling","title":"lr_scaling","text":"<pre><code>lr_scaling: dict[str, float]\n</code></pre> <p>Compute the learning rate scaling for the covariance parameters.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.output_rank","title":"output_rank","text":"<pre><code>output_rank = output_rank\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.rank","title":"rank","text":"<pre><code>rank = rank\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.sample_scale","title":"sample_scale","text":"<pre><code>sample_scale: float\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.factor_matmul","title":"factor_matmul","text":"<pre><code>factor_matmul(\n    input: Float[Tensor, \"*sample parameter\"],\n    /,\n    additive_constant: (\n        Float[Tensor, \"*sample parameter\"] | None\n    ) = None,\n) -&gt; dict[str, Float[Tensor, \"*sample parameter\"]]\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.initialize_parameters","title":"initialize_parameters","text":"<pre><code>initialize_parameters(\n    mean_parameters: dict[str, Tensor],\n) -&gt; None\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters(\n    mean_parameter_scales: dict[str, float] | float = 1.0,\n) -&gt; None\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.KroneckerCovariance.to_dense","title":"to_dense","text":"<pre><code>to_dense() -&gt; Float[Tensor, 'parameter parameter']\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.LowRankCovariance","title":"LowRankCovariance","text":"<pre><code>LowRankCovariance(rank: int)\n</code></pre> <p>               Bases: <code>FactorizedCovariance</code></p> <p>Covariance of a Gaussian parameter with low-rank structure.</p> <p>Assumes the covariance is factorized as a product of a matrix :math:\\mathbf{S} \\in \\mathbb{R}^{P \\times R}` and its transpose.</p> <p>..math::     \\mathbf{\\Sigma} = \\mathbf{S} \\mathbf{S}^\\top</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>Rank of the covariance matrix. If <code>None</code>, the rank is set to the total number of mean parameters.</p> required <p>Methods:</p> Name Description <code>factor_matmul</code> <p>Multiply left factor of the covariance matrix with the input.</p> <code>initialize_parameters</code> <p>Initialize the covariance parameters.</p> <code>reset_parameters</code> <p>Reset the parameters of the covariance matrix.</p> <code>to_dense</code> <p>Convert the covariance matrix to a dense representation.</p> <p>Attributes:</p> Name Type Description <code>lr_scaling</code> <code>dict[str, float]</code> <p>Compute the learning rate scaling for the covariance parameters.</p> <code>rank</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.LowRankCovariance.lr_scaling","title":"lr_scaling","text":"<pre><code>lr_scaling: dict[str, float]\n</code></pre> <p>Compute the learning rate scaling for the covariance parameters.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.LowRankCovariance.rank","title":"rank","text":"<pre><code>rank = rank\n</code></pre>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.LowRankCovariance.factor_matmul","title":"factor_matmul","text":"<pre><code>factor_matmul(\n    input: Float[Tensor, \"*sample parameter\"],\n    /,\n    additive_constant: (\n        Float[Tensor, \"*sample parameter\"] | None\n    ) = None,\n) -&gt; dict[str, Float[Tensor, \"*sample parameter\"]]\n</code></pre> <p>Multiply left factor of the covariance matrix with the input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Float[Tensor, '*sample parameter']</code> <p>Input tensor.</p> required <code>additive_constant</code> <code>Float[Tensor, '*sample parameter'] | None</code> <p>Additive constant to be added to the output.</p> <code>None</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.LowRankCovariance.initialize_parameters","title":"initialize_parameters","text":"<pre><code>initialize_parameters(\n    mean_parameters: dict[str, Tensor],\n) -&gt; None\n</code></pre> <p>Initialize the covariance parameters.</p> <p>Parameters:</p> Name Type Description Default <code>mean_parameters</code> <code>dict[str, Tensor]</code> <p>Mean parameters of the Gaussian distribution.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Covariance parameters.</p>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.LowRankCovariance.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters(\n    mean_parameter_scales: dict[str, float] | float = 1.0,\n) -&gt; None\n</code></pre> <p>Reset the parameters of the covariance matrix.</p> <p>Initalizes the parameters of the covariance matrix with a scale that is given by the mean parameter scales and a covariance-specific scaling that depends on the structure of the covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>mean_parameter_scales</code> <code>dict[str, float] | float</code> <p>Scales of the mean parameters. If a dictionary keys are the names of the mean parameters. If a float, all covariance parameters are initialized with the same scale.</p> <code>1.0</code>"},{"location":"api/bnn/bnn.params/parameters/#inferno.bnn.params.LowRankCovariance.to_dense","title":"to_dense","text":"<pre><code>to_dense() -&gt; Float[Tensor, 'parameter parameter']\n</code></pre> <p>Convert the covariance matrix to a dense representation.</p>"},{"location":"api/bnn/bnn.params/parametrizations/","title":"Parametrizations","text":""},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations","title":"parametrizations","text":"<p>Classes:</p> Name Description <code>Parametrization</code> <p>Abstract base class for all neural network parametrizations.</p> <code>Standard</code> <p>Standard Parametrization (SP).</p> <code>NeuralTangent</code> <p>Neural Tangent Parametrization (NTP).</p> <code>MaximalUpdate</code> <p>Maximal update parametrization (\\(\\mu P\\)).</p>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Parametrization","title":"Parametrization","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all neural network parametrizations.</p> <p>Methods:</p> Name Description <code>bias_init_scale</code> <p>Compute the bias initialization scale for the given layer.</p> <code>bias_lr_scale</code> <p>Compute the learning rate scale for the bias parameters.</p> <code>weight_init_scale</code> <p>Compute the weight initialization scale for the given layer.</p> <code>weight_lr_scale</code> <p>Compute the learning rate scale for the weight parameters.</p>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Parametrization.bias_init_scale","title":"bias_init_scale","text":"<pre><code>bias_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre> <p>Compute the bias initialization scale for the given layer.</p> <p>Parameters:</p> Name Type Description Default <code>fan_in</code> <code>int</code> <p>Number of inputs to the layer.</p> required <code>fan_out</code> <code>int</code> <p>Number of outputs from the layer.</p> required <code>layer_type</code> <code>LayerType</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\".</p> <code>'hidden'</code>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Parametrization.bias_lr_scale","title":"bias_lr_scale","text":"<pre><code>bias_lr_scale(\n    fan_in: int,\n    fan_out: int,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre> <p>Compute the learning rate scale for the bias parameters.</p> <p>Parameters:</p> Name Type Description Default <code>fan_in</code> <code>int</code> <p>Number of inputs to the layer.</p> required <code>fan_out</code> <code>int</code> <p>Number of outputs from the layer.</p> required <code>optimizer</code> <code>str</code> <p>Optimizer being used.</p> required <code>layer_type</code> <code>LayerType</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\".</p> <code>'hidden'</code>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Parametrization.weight_init_scale","title":"weight_init_scale","text":"<pre><code>weight_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre> <p>Compute the weight initialization scale for the given layer.</p> <p>Parameters:</p> Name Type Description Default <code>fan_in</code> <code>int</code> <p>Number of inputs to the layer.</p> required <code>fan_out</code> <code>int</code> <p>Number of outputs from the layer.</p> required <code>layer_type</code> <code>LayerType</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\".</p> <code>'hidden'</code>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Parametrization.weight_lr_scale","title":"weight_lr_scale","text":"<pre><code>weight_lr_scale(\n    fan_in: int,\n    fan_out: int,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre> <p>Compute the learning rate scale for the weight parameters.</p> <p>Parameters:</p> Name Type Description Default <code>fan_in</code> <code>int</code> <p>Number of inputs to the layer.</p> required <code>fan_out</code> <code>int</code> <p>Number of outputs from the layer.</p> required <code>optimizer</code> <code>str</code> <p>Optimizer being used.</p> required <code>layer_type</code> <code>LayerType</code> <p>Type of the layer. Can be one of \"input\", \"hidden\", or \"output\".</p> <code>'hidden'</code>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Standard","title":"Standard","text":"<p>               Bases: <code>Parametrization</code></p> <p>Standard Parametrization (SP).</p> <p>The default parametrization for neural networks in PyTorch. Also known as the 'fan_in' or 'LeCun' initialization. 'Kaiming' initialization is the same up to multiplicative constants.</p> <p>Methods:</p> Name Description <code>bias_init_scale</code> <code>bias_lr_scale</code> <code>weight_init_scale</code> <code>weight_lr_scale</code>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Standard.bias_init_scale","title":"bias_init_scale","text":"<pre><code>bias_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Standard.bias_lr_scale","title":"bias_lr_scale","text":"<pre><code>bias_lr_scale(\n    fan_in,\n    fan_out,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n)\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Standard.weight_init_scale","title":"weight_init_scale","text":"<pre><code>weight_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.Standard.weight_lr_scale","title":"weight_lr_scale","text":"<pre><code>weight_lr_scale(\n    fan_in: int,\n    fan_out: int,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.NeuralTangent","title":"NeuralTangent","text":"<p>               Bases: <code>Parametrization</code></p> <p>Neural Tangent Parametrization (NTP).</p> <p>The neural tangent parametrization enables the theoretical analysis of the training dynamics of infinite-width neural networks (<code>Jacot et al., 2018</code>_) via the neural tangent kernel. However, NTP does not admit feature learning, as features are effectively fixed at initialization.</p> <p>.. _Jacot et al., 2018: http://arxiv.org/abs/1806.07572</p> <p>Methods:</p> Name Description <code>bias_init_scale</code> <code>bias_lr_scale</code> <code>weight_init_scale</code> <code>weight_lr_scale</code>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.NeuralTangent.bias_init_scale","title":"bias_init_scale","text":"<pre><code>bias_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.NeuralTangent.bias_lr_scale","title":"bias_lr_scale","text":"<pre><code>bias_lr_scale(\n    fan_in: int,\n    fan_out: int,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.NeuralTangent.weight_init_scale","title":"weight_init_scale","text":"<pre><code>weight_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.NeuralTangent.weight_lr_scale","title":"weight_lr_scale","text":"<pre><code>weight_lr_scale(\n    fan_in: int,\n    fan_out: int,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.MaximalUpdate","title":"MaximalUpdate","text":"<p>               Bases: <code>Parametrization</code></p> <p>Maximal update parametrization (\\(\\mu P\\)).</p> <p>The maximal update parametrization (<code>Yang et al., 2021</code>, <code>Yang et al., 2021b</code>) enforces:     - stable training dynamics, meaning (pre)activations and logits at activations are independent       of width at initialization and features and logits do not explode during training.     - feature learning, meaning the model can learn representations from the data at any width.</p> <p>This parametrization is particularly useful when training large models, since it maintains stable training dynamics at any width and enables hyperparameter transfer. This means one can tune the learning rate on a smaller model and achieve good generalization with the tuned learning rate for a large model.</p> <p>.. _Yang et al., 2021: http://arxiv.org/abs/2011.14522 .. _Yang et al., 2021b: http://arxiv.org/abs/2203.03466</p> <p>Methods:</p> Name Description <code>bias_init_scale</code> <code>bias_lr_scale</code> <code>weight_init_scale</code> <code>weight_lr_scale</code>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.MaximalUpdate.bias_init_scale","title":"bias_init_scale","text":"<pre><code>bias_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.MaximalUpdate.bias_lr_scale","title":"bias_lr_scale","text":"<pre><code>bias_lr_scale(\n    fan_in: int,\n    fan_out: int,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.MaximalUpdate.weight_init_scale","title":"weight_init_scale","text":"<pre><code>weight_init_scale(\n    fan_in: int,\n    fan_out: int,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/bnn/bnn.params/parametrizations/#inferno.bnn.params.parametrizations.MaximalUpdate.weight_lr_scale","title":"weight_lr_scale","text":"<pre><code>weight_lr_scale(\n    fan_in: int,\n    fan_out: int,\n    optimizer: str,\n    layer_type: LayerType = \"hidden\",\n) -&gt; float\n</code></pre>"},{"location":"api/datasets/image/","title":"Image","text":""},{"location":"api/datasets/image/#inferno.datasets","title":"datasets &gt; Image Data","text":"<p>Classes:</p> Name Description <code>CIFAR10C</code> <p>Corrupted CIFAR10 image classification dataset.</p> <code>CIFAR100C</code> <p>Corrupted CIFAR100 image classification dataset.</p> <code>MNISTC</code> <p>Corrupted MNIST image classification dataset.</p> <code>TinyImageNet</code> <p>TinyImageNet image classification dataset.</p> <code>TinyImageNetC</code> <p>Corrupted TinyImageNet image classification dataset.</p>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR10C","title":"CIFAR10C","text":"<pre><code>CIFAR10C(\n    root: Path | str = None,\n    transform: Callable | None = None,\n    target_transform: Callable | None = None,\n    corruptions: list[str] = [\n        \"brightness\",\n        \"contrast\",\n        \"defocus_blur\",\n        \"elastic_transform\",\n        \"fog\",\n        \"frost\",\n        \"gaussian_blur\",\n        \"gaussian_noise\",\n        \"glass_blur\",\n        \"impulse_noise\",\n        \"jpeg_compression\",\n        \"motion_blur\",\n        \"pixelate\",\n        \"saturate\",\n        \"shot_noise\",\n        \"snow\",\n        \"spatter\",\n        \"speckle_noise\",\n        \"zoom_blur\",\n    ],\n    shift_severity: int = 5,\n    download: bool = False,\n)\n</code></pre> <p>               Bases: <code>VisionDataset</code></p> <p>Corrupted CIFAR10 image classification dataset.</p> <p>Contains 10,000 test images for each corruption. From Benchmarking Neural Network Robustness to Common Corruptions and Perturbations.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Path | str</code> <p>Root directory of the dataset.</p> <code>None</code> <code>transform</code> <code>Callable | None</code> <p>Transform to apply to the data.</p> <code>None</code> <code>target_transform</code> <code>Callable | None</code> <p>Transform to apply to the targets.</p> <code>None</code> <code>corruptions</code> <code>list[str]</code> <p>List of corruptions to apply to the data.</p> <code>['brightness', 'contrast', 'defocus_blur', 'elastic_transform', 'fog', 'frost', 'gaussian_blur', 'gaussian_noise', 'glass_blur', 'impulse_noise', 'jpeg_compression', 'motion_blur', 'pixelate', 'saturate', 'shot_noise', 'snow', 'spatter', 'speckle_noise', 'zoom_blur']</code> <code>shift_severity</code> <code>int</code> <p>Severity of the corruption to apply. Must be an integer between 1 and 5.</p> <code>5</code> <code>download</code> <code>bool</code> <p>If true, downloads the dataset from the internet and puts it in the root directory.</p> <code>False</code> <p>Methods:</p> Name Description <code>download</code> <p>Attributes:</p> Name Type Description <code>base_folder</code> <code>corruption_data_checksums</code> <code>corruptions</code> <code>data</code> <code>filename</code> <code>shift_severity</code> <code>sub_folder</code> <code>targets</code> <code>tgz_md5</code> <code>url</code>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR10C.base_folder","title":"base_folder","text":"<pre><code>base_folder = Path('CIFAR10-C/raw')\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR10C.corruption_data_checksums","title":"corruption_data_checksums","text":"<pre><code>corruption_data_checksums = {\n    \"fog\": \"7b397314b5670f825465fbcd1f6e9ccd\",\n    \"jpeg_compression\": \"2b9cc4c864e0193bb64db8d7728f8187\",\n    \"zoom_blur\": \"6ea8e63f1c5cdee1517533840641641b\",\n    \"speckle_noise\": \"ef00b87611792b00df09c0b0237a1e30\",\n    \"glass_blur\": \"7361fb4019269e02dbf6925f083e8629\",\n    \"spatter\": \"8a5a3903a7f8f65b59501a6093b4311e\",\n    \"shot_noise\": \"3a7239bb118894f013d9bf1984be7f11\",\n    \"defocus_blur\": \"7d1322666342a0702b1957e92f6254bc\",\n    \"elastic_transform\": \"9421657c6cd452429cf6ce96cc412b5f\",\n    \"gaussian_blur\": \"c33370155bc9b055fb4a89113d3c559d\",\n    \"frost\": \"31f6ab3bce1d9934abfb0cc13656f141\",\n    \"saturate\": \"1cfae0964219c5102abbb883e538cc56\",\n    \"brightness\": \"0a81ef75e0b523c3383219c330a85d48\",\n    \"snow\": \"bb238de8555123da9c282dea23bd6e55\",\n    \"gaussian_noise\": \"ecaf8b9a2399ffeda7680934c33405fd\",\n    \"motion_blur\": \"fffa5f852ff7ad299cfe8a7643f090f4\",\n    \"contrast\": \"3c8262171c51307f916c30a3308235a8\",\n    \"impulse_noise\": \"2090e01c83519ec51427e65116af6b1a\",\n    \"labels\": \"c439b113295ed5254878798ffe28fd54\",\n    \"pixelate\": \"0f14f7e2db14288304e1de10df16832f\",\n}\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR10C.corruptions","title":"corruptions","text":"<pre><code>corruptions = corruptions\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR10C.data","title":"data","text":"<pre><code>data = concatenate(\n    [\n        (\n            load(root / sub_folder / (corruption + \".npy\"))[\n                ((shift_severity - 1) * 10000) : (\n                    shift_severity * 10000\n                )\n            ]\n        )\n        for corruption in (corruptions)\n    ],\n    axis=0,\n)\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR10C.filename","title":"filename","text":"<pre><code>filename = 'CIFAR-10-C.tar'\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR10C.shift_severity","title":"shift_severity","text":"<pre><code>shift_severity = shift_severity\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR10C.sub_folder","title":"sub_folder","text":"<pre><code>sub_folder = Path('CIFAR-10-C')\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR10C.targets","title":"targets","text":"<pre><code>targets = astype(int64)\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR10C.tgz_md5","title":"tgz_md5","text":"<pre><code>tgz_md5 = '56bf5dcef84df0e2308c6dcbcbbd8499'\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR10C.url","title":"url","text":"<pre><code>url = (\n    \"https://zenodo.org/record/2535967/files/CIFAR-10-C.tar\"\n)\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR10C.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR100C","title":"CIFAR100C","text":"<pre><code>CIFAR100C(\n    root: Path | str = None,\n    transform: Callable | None = None,\n    target_transform: Callable | None = None,\n    corruptions: list[str] = [\n        \"brightness\",\n        \"contrast\",\n        \"defocus_blur\",\n        \"elastic_transform\",\n        \"fog\",\n        \"frost\",\n        \"gaussian_blur\",\n        \"gaussian_noise\",\n        \"glass_blur\",\n        \"impulse_noise\",\n        \"jpeg_compression\",\n        \"motion_blur\",\n        \"pixelate\",\n        \"saturate\",\n        \"shot_noise\",\n        \"snow\",\n        \"spatter\",\n        \"speckle_noise\",\n        \"zoom_blur\",\n    ],\n    shift_severity: int = 5,\n    download: bool = False,\n)\n</code></pre> <p>               Bases: <code>CIFAR10C</code></p> <p>Corrupted CIFAR100 image classification dataset.</p> <p>From Benchmarking Neural Network Robustness to Common Corruptions and Perturbations.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Path | str</code> <p>Root directory of the dataset.</p> <code>None</code> <code>transform</code> <code>Callable | None</code> <p>Transform to apply to the data.</p> <code>None</code> <code>target_transform</code> <code>Callable | None</code> <p>Transform to apply to the targets.</p> <code>None</code> <code>corruptions</code> <code>list[str]</code> <p>List of corruptions to apply to the data.</p> <code>['brightness', 'contrast', 'defocus_blur', 'elastic_transform', 'fog', 'frost', 'gaussian_blur', 'gaussian_noise', 'glass_blur', 'impulse_noise', 'jpeg_compression', 'motion_blur', 'pixelate', 'saturate', 'shot_noise', 'snow', 'spatter', 'speckle_noise', 'zoom_blur']</code> <code>shift_severity</code> <code>int</code> <p>Severity of the corruption to apply. Must be an integer between 1 and 5.</p> <code>5</code> <code>download</code> <code>bool</code> <p>If true, downloads the dataset from the internet and puts it in root directory.</p> <code>False</code> <p>Methods:</p> Name Description <code>download</code> <p>Attributes:</p> Name Type Description <code>base_folder</code> <code>corruption_data_checksums</code> <code>corruptions</code> <code>data</code> <code>filename</code> <code>shift_severity</code> <code>sub_folder</code> <code>targets</code> <code>tgz_md5</code> <code>url</code>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR100C.base_folder","title":"base_folder","text":"<pre><code>base_folder = Path('CIFAR100-C/raw')\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR100C.corruption_data_checksums","title":"corruption_data_checksums","text":"<pre><code>corruption_data_checksums = {\n    \"fog\": \"4efc7ebd5e82b028bdbe13048e3ea564\",\n    \"jpeg_compression\": \"c851b7f1324e1d2ffddeb76920576d11\",\n    \"zoom_blur\": \"0204613400c034a81c4830d5df81cb82\",\n    \"speckle_noise\": \"e3f215b1a0f9fd9fd6f0d1cf94a7ce99\",\n    \"glass_blur\": \"0bf384f38e5ccbf8dd479d9059b913e1\",\n    \"spatter\": \"12ccf41d62564d36e1f6a6ada5022728\",\n    \"shot_noise\": \"b0a1fa6e1e465a747c1b204b1914048a\",\n    \"defocus_blur\": \"d923e3d9c585a27f0956e2f2ad832564\",\n    \"elastic_transform\": \"a0792bd6581f6810878be71acedfc65a\",\n    \"gaussian_blur\": \"5204ba0d557839772ef5a4196a052c3e\",\n    \"frost\": \"3a39c6823bdfaa0bf8b12fe7004b8117\",\n    \"saturate\": \"c0697e9fdd646916a61e9c312c77bf6b\",\n    \"brightness\": \"f22d7195aecd6abb541e27fca230c171\",\n    \"snow\": \"0237be164583af146b7b144e73b43465\",\n    \"gaussian_noise\": \"ecc4d366eac432bdf25c024086f5e97d\",\n    \"motion_blur\": \"732a7e2e54152ff97c742d4c388c5516\",\n    \"contrast\": \"322bb385f1d05154ee197ca16535f71e\",\n    \"impulse_noise\": \"3b3c210ddfa0b5cb918ff4537a429fef\",\n    \"labels\": \"bb4026e9ce52996b95f439544568cdb2\",\n    \"pixelate\": \"96c00c60f144539e14cffb02ddbd0640\",\n}\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR100C.corruptions","title":"corruptions","text":"<pre><code>corruptions = corruptions\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR100C.data","title":"data","text":"<pre><code>data = concatenate(\n    [\n        (\n            load(root / sub_folder / (corruption + \".npy\"))[\n                ((shift_severity - 1) * 10000) : (\n                    shift_severity * 10000\n                )\n            ]\n        )\n        for corruption in (corruptions)\n    ],\n    axis=0,\n)\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR100C.filename","title":"filename","text":"<pre><code>filename = 'CIFAR-100-C.tar'\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR100C.shift_severity","title":"shift_severity","text":"<pre><code>shift_severity = shift_severity\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR100C.sub_folder","title":"sub_folder","text":"<pre><code>sub_folder = Path('CIFAR-100-C')\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR100C.targets","title":"targets","text":"<pre><code>targets = astype(int64)\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR100C.tgz_md5","title":"tgz_md5","text":"<pre><code>tgz_md5 = '11f0ed0f1191edbf9fa23466ae6021d3'\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR100C.url","title":"url","text":"<pre><code>url = \"https://zenodo.org/record/3555552/files/CIFAR-100-C.tar\"\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.CIFAR100C.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.MNISTC","title":"MNISTC","text":"<pre><code>MNISTC(\n    root: Path | str = None,\n    transform: Callable | None = None,\n    target_transform: Callable | None = None,\n    corruptions: list[str] = [\n        \"brightness\",\n        \"canny_edges\",\n        \"dotted_line\",\n        \"fog\",\n        \"glass_blur\",\n        \"impulse_noise\",\n        \"motion_blur\",\n        \"rotate\",\n        \"scale\",\n        \"shear\",\n        \"shot_noise\",\n        \"spatter\",\n        \"stripe\",\n        \"translate\",\n        \"zigzag\",\n    ],\n    download: bool = False,\n)\n</code></pre> <p>               Bases: <code>VisionDataset</code></p> <p>Corrupted MNIST image classification dataset.</p> <p>Contains 10,000 test images for each one of 15 corruptions. From MNIST-C: A Robustness Benchmark for Computer Vision.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Path | str</code> <p>Root directory of the dataset.</p> <code>None</code> <code>transform</code> <code>Callable | None</code> <p>Transform to apply to the data.</p> <code>None</code> <code>target_transform</code> <code>Callable | None</code> <p>Transform to apply to the targets.</p> <code>None</code> <code>corruptions</code> <code>list[str]</code> <p>List of corruptions to apply to the data.</p> <code>['brightness', 'canny_edges', 'dotted_line', 'fog', 'glass_blur', 'impulse_noise', 'motion_blur', 'rotate', 'scale', 'shear', 'shot_noise', 'spatter', 'stripe', 'translate', 'zigzag']</code> <code>download</code> <code>bool</code> <p>If true, downloads the dataset from the internet and puts it in the root directory.</p> <code>False</code> <p>Methods:</p> Name Description <code>download</code> <p>Attributes:</p> Name Type Description <code>base_folder</code> <code>corruptions</code> <code>data</code> <code>filename</code> <code>sub_folder</code> <code>targets</code> <code>url</code> <code>zip_md5</code>"},{"location":"api/datasets/image/#inferno.datasets.MNISTC.base_folder","title":"base_folder","text":"<pre><code>base_folder = Path('MNIST-C/raw')\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.MNISTC.corruptions","title":"corruptions","text":"<pre><code>corruptions = corruptions\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.MNISTC.data","title":"data","text":"<pre><code>data = concatenate(\n    [\n        (\n            load(\n                root\n                / sub_folder\n                / corruption\n                / \"test_images.npy\"\n            )\n        )\n        for corruption in (corruptions)\n    ],\n    axis=0,\n)\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.MNISTC.filename","title":"filename","text":"<pre><code>filename = 'mnist_c.zip'\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.MNISTC.sub_folder","title":"sub_folder","text":"<pre><code>sub_folder = Path('mnist_c')\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.MNISTC.targets","title":"targets","text":"<pre><code>targets = astype(int64)\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.MNISTC.url","title":"url","text":"<pre><code>url = 'https://zenodo.org/record/3239543/files/mnist_c.zip'\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.MNISTC.zip_md5","title":"zip_md5","text":"<pre><code>zip_md5 = '4b34b33045869ee6d424616cd3a65da3'\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.MNISTC.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNet","title":"TinyImageNet","text":"<pre><code>TinyImageNet(\n    root: str | Path,\n    train: bool = True,\n    transform: Callable | None = None,\n    target_transform: Callable | None = None,\n    download: bool = False,\n)\n</code></pre> <p>               Bases: <code>VisionDataset</code></p> <p>TinyImageNet image classification dataset.</p> <p>The training dataset contains 100,000 images of 200 classes (500 for each class) downsized to 64x64 color images. The test set has 10,000 images (50 for each class).</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str | Path</code> <p>Root directory of the dataset.</p> required <code>train</code> <code>bool</code> <p>If True, creates dataset from training data, otherwise from test data.</p> <code>True</code> <code>transform</code> <code>Callable | None</code> <p>Transform to apply to the data.</p> <code>None</code> <code>target_transform</code> <code>Callable | None</code> <p>Transform to apply to the targets.</p> <code>None</code> <code>download</code> <code>bool</code> <p>If true, downloads the dataset from the internet and puts it in the root directory.</p> <code>False</code> <p>Methods:</p> Name Description <code>download</code> <p>Attributes:</p> Name Type Description <code>base_folder</code> <code>class_to_idx</code> <code>data</code> <code>filename</code> <code>targets</code> <code>tgz_md5</code> <code>url</code>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNet.base_folder","title":"base_folder","text":"<pre><code>base_folder = Path('TinyImageNet/raw')\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNet.class_to_idx","title":"class_to_idx","text":"<pre><code>class_to_idx = {\n    (classes[i]): i for i in (range(len(classes)))\n}\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNet.data","title":"data","text":"<pre><code>data = []\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNet.filename","title":"filename","text":"<pre><code>filename = 'tiny-imagenet-200.zip'\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNet.targets","title":"targets","text":"<pre><code>targets = [(item[1]) for item in (data)]\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNet.tgz_md5","title":"tgz_md5","text":"<pre><code>tgz_md5 = '90528d7ca1a48142e341f4ef8d21d0de'\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNet.url","title":"url","text":"<pre><code>url = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNet.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNetC","title":"TinyImageNetC","text":"<pre><code>TinyImageNetC(\n    root: Path | str = None,\n    transform: Callable | None = None,\n    target_transform: Callable | None = None,\n    corruptions: list[str] = [\n        \"brightness\",\n        \"contrast\",\n        \"defocus_blur\",\n        \"elastic_transform\",\n        \"fog\",\n        \"frost\",\n        \"gaussian_blur\",\n        \"gaussian_noise\",\n        \"glass_blur\",\n        \"impulse_noise\",\n        \"jpeg_compression\",\n        \"motion_blur\",\n        \"pixelate\",\n        \"saturate\",\n        \"shot_noise\",\n        \"snow\",\n        \"spatter\",\n        \"speckle_noise\",\n        \"zoom_blur\",\n    ],\n    shift_severity: int = 5,\n    download: bool = False,\n)\n</code></pre> <p>               Bases: <code>VisionDataset</code></p> <p>Corrupted TinyImageNet image classification dataset.</p> <p>Contains 10,000 64x64 color test images for each corruption (200 classes, 50 images per class). From Benchmarking Neural Network Robustness to Common Corruptions and Perturbations.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Path | str</code> <p>Root directory of the dataset.</p> <code>None</code> <code>transform</code> <code>Callable | None</code> <p>Transform to apply to the data.</p> <code>None</code> <code>target_transform</code> <code>Callable | None</code> <p>Transform to apply to the targets.</p> <code>None</code> <code>corruptions</code> <code>list[str]</code> <p>List of corruptions to apply to the data.</p> <code>['brightness', 'contrast', 'defocus_blur', 'elastic_transform', 'fog', 'frost', 'gaussian_blur', 'gaussian_noise', 'glass_blur', 'impulse_noise', 'jpeg_compression', 'motion_blur', 'pixelate', 'saturate', 'shot_noise', 'snow', 'spatter', 'speckle_noise', 'zoom_blur']</code> <code>shift_severity</code> <code>int</code> <p>Severity of the corruption to apply. Must be an integer between 1 and 5.</p> <code>5</code> <code>download</code> <code>bool</code> <p>If true, downloads the dataset from the internet and puts it in the root directory.</p> <code>False</code> <p>Methods:</p> Name Description <code>download</code> <p>Download the dataset.</p> <p>Attributes:</p> Name Type Description <code>base_folder</code> <code>corruptions</code> <code>data</code> <code>filename</code> <code>shift_severity</code> <code>targets</code> <code>tgz_md5</code> <code>url</code>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNetC.base_folder","title":"base_folder","text":"<pre><code>base_folder = Path('TinyImageNet-C/raw')\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNetC.corruptions","title":"corruptions","text":"<pre><code>corruptions = corruptions\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNetC.data","title":"data","text":"<pre><code>data = []\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNetC.filename","title":"filename","text":"<pre><code>filename = [\n    \"Tiny-ImageNet-C.tar\",\n    \"Tiny-ImageNet-C-extra.tar\",\n]\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNetC.shift_severity","title":"shift_severity","text":"<pre><code>shift_severity = shift_severity\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNetC.targets","title":"targets","text":"<pre><code>targets = []\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNetC.tgz_md5","title":"tgz_md5","text":"<pre><code>tgz_md5 = [\n    \"f9c9a9dbdc11469f0b850190f7ad8be1\",\n    \"0db0588d243cf403ef93449ec52b70eb\",\n]\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNetC.url","title":"url","text":"<pre><code>url = 'https://zenodo.org/record/8206060/files/'\n</code></pre>"},{"location":"api/datasets/image/#inferno.datasets.TinyImageNetC.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre> <p>Download the dataset.</p>"},{"location":"api/datasets/tabular/","title":"Tabular","text":""},{"location":"api/datasets/tabular/#inferno.datasets","title":"datasets &gt; Tabular Data","text":"<p>Classes:</p> Name Description <code>ConcreteCompressiveStrength</code> <p>Concrete compressive strength (1,030 \u00d7 8).</p> <code>ParkinsonsTelemonitoring</code> <p>Parkinsons telemonitoring (5,875 \u00d7 21).</p> <code>ProteinStructure</code> <p>Physicochemical properties of protein tertiary structure (45,730 \u00d7 9).</p> <code>RoadNetwork</code> <p>3D Road Network (434,874 \u00d7 2).</p> <code>WineQuality</code> <p>Wine quality prediction from physicochemical properties (4,898 \u00d7 11).</p>"},{"location":"api/datasets/tabular/#inferno.datasets.ConcreteCompressiveStrength","title":"ConcreteCompressiveStrength","text":"<pre><code>ConcreteCompressiveStrength(\n    root: str | Path = None,\n    transform: Callable | None = Lambda(\n        lambda x: (\n            x\n            - as_tensor(\n                [\n                    281.1656,\n                    73.8955,\n                    54.1871,\n                    181.5664,\n                    6.2031,\n                    972.9186,\n                    773.5789,\n                    45.6621,\n                ]\n            )\n        )\n        / as_tensor(\n            [\n                104.5071,\n                86.2791,\n                63.9965,\n                21.3556,\n                5.9735,\n                77.7538,\n                80.1754,\n                63.1699,\n            ]\n        )\n    ),\n    target_transform: Callable | None = Lambda(\n        lambda y: (y - 35.8178) / 16.7057\n    ),\n    download: bool = False,\n)\n</code></pre> <p>               Bases: <code>RegressionDataset</code></p> <p>Concrete compressive strength (1,030 \u00d7 8).</p> <p>This UCI dataset contains the ingredients of concrete mixtures and their age. The regression task is to predict the concrete's compressive strength.</p> <p>Source: https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength</p> <p>Methods:</p> Name Description <code>download</code> <p>Attributes:</p> Name Type Description <code>URL</code> <code>filepath</code> <code>str</code> <code>md5</code> <code>root</code> <code>target_transform</code> <code>transform</code>"},{"location":"api/datasets/tabular/#inferno.datasets.ConcreteCompressiveStrength.URL","title":"URL","text":"<pre><code>URL = \"https://archive.ics.uci.edu/static/public/165/concrete+compressive+strength.zip\"\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ConcreteCompressiveStrength.filepath","title":"filepath","text":"<pre><code>filepath: str\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ConcreteCompressiveStrength.md5","title":"md5","text":"<pre><code>md5 = '4aaeecaf0bf2eefccb8a4a6d4cc12785'\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ConcreteCompressiveStrength.root","title":"root","text":"<pre><code>root = Path(root)\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ConcreteCompressiveStrength.target_transform","title":"target_transform","text":"<pre><code>target_transform = target_transform\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ConcreteCompressiveStrength.transform","title":"transform","text":"<pre><code>transform = transform\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ConcreteCompressiveStrength.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ParkinsonsTelemonitoring","title":"ParkinsonsTelemonitoring","text":"<pre><code>ParkinsonsTelemonitoring(\n    root: str | Path = None,\n    transform: Callable | None = Lambda(\n        lambda x: (\n            x\n            - as_tensor(\n                [\n                    21.494,\n                    64.805,\n                    0.31779,\n                    92.864,\n                    21.296,\n                    0.0061538,\n                    4.4027e-05,\n                    0.0029872,\n                    0.0032769,\n                    0.0089617,\n                    0.034035,\n                    0.31096,\n                    0.017156,\n                    0.020144,\n                    0.027481,\n                    0.051467,\n                    0.03212,\n                    21.679,\n                    0.54147,\n                    0.65324,\n                    0.21959,\n                ]\n            )\n        )\n        / as_tensor(\n            [\n                12.372,\n                8.8215,\n                0.46566,\n                53.446,\n                8.1293,\n                0.0056242,\n                3.5983e-05,\n                0.0031238,\n                0.0037315,\n                0.0093715,\n                0.025835,\n                0.23025,\n                0.013237,\n                0.016664,\n                0.019986,\n                0.039711,\n                0.059692,\n                4.2911,\n                0.10099,\n                0.070902,\n                0.091498,\n            ]\n        )\n    ),\n    target_transform: Callable | None = Lambda(\n        lambda y: (y - 29.0189) / 10.7003\n    ),\n    download: bool = False,\n)\n</code></pre> <p>               Bases: <code>RegressionDataset</code></p> <p>Parkinsons telemonitoring (5,875 \u00d7 21).</p> <p>This UCI dataset is composed of a range of biomedical voice measurements from 42 people with early-stage Parkinson's disease recruited to a six-month trial of a telemonitoring device for remote symptom progression monitoring. The recordings were automatically captured in the patient's homes. The original study used a range of linear and nonlinear regression methods to predict the clinician's Parkinson's disease symptom score on the UPDRS scale.</p> <p>Source: https://archive.ics.uci.edu/ml/datasets/parkinsons+telemonitoring</p> <p>Methods:</p> Name Description <code>download</code> <p>Attributes:</p> Name Type Description <code>URL</code> <code>filepath</code> <code>str</code> <code>md5</code> <code>root</code> <code>target_transform</code> <code>transform</code>"},{"location":"api/datasets/tabular/#inferno.datasets.ParkinsonsTelemonitoring.URL","title":"URL","text":"<pre><code>URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/\"\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ParkinsonsTelemonitoring.filepath","title":"filepath","text":"<pre><code>filepath: str\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ParkinsonsTelemonitoring.md5","title":"md5","text":"<pre><code>md5 = 'eba8e7531ac24fbe8473085a0a48e556'\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ParkinsonsTelemonitoring.root","title":"root","text":"<pre><code>root = Path(root)\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ParkinsonsTelemonitoring.target_transform","title":"target_transform","text":"<pre><code>target_transform = target_transform\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ParkinsonsTelemonitoring.transform","title":"transform","text":"<pre><code>transform = transform\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ParkinsonsTelemonitoring.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ProteinStructure","title":"ProteinStructure","text":"<pre><code>ProteinStructure(\n    root: str | Path = None,\n    transform: Callable | None = Lambda(\n        lambda x: (\n            x\n            - as_tensor(\n                [\n                    9871.6,\n                    3017.4,\n                    0.30239,\n                    103.49,\n                    1368300.0,\n                    145.64,\n                    3989.8,\n                    69.975,\n                    34.524,\n                ]\n            )\n        )\n        / as_tensor(\n            [\n                4058.1,\n                1464.3,\n                0.062886,\n                55.425,\n                564040.0,\n                69.999,\n                1993.6,\n                56.493,\n                5.9798,\n            ]\n        )\n    ),\n    target_transform: Callable | None = Lambda(\n        lambda y: (y - 7.7485) / 6.1183\n    ),\n    download: bool = False,\n)\n</code></pre> <p>               Bases: <code>RegressionDataset</code></p> <p>Physicochemical properties of protein tertiary structure (45,730 \u00d7 9).</p> <p>This UCI dataset encompasses the physicochemical properties of protein tertiary structure, sourced from CASP 5-9. There are 45,730 decoys with 9 attributes and sizes varying from 0 to 21 angstroms.</p> <p>Source: https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure</p> <p>Methods:</p> Name Description <code>download</code> <p>Attributes:</p> Name Type Description <code>URL</code> <code>filepath</code> <code>str</code> <code>md5</code> <code>root</code> <code>target_transform</code> <code>transform</code>"},{"location":"api/datasets/tabular/#inferno.datasets.ProteinStructure.URL","title":"URL","text":"<pre><code>URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00265/\"\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ProteinStructure.filepath","title":"filepath","text":"<pre><code>filepath: str\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ProteinStructure.md5","title":"md5","text":"<pre><code>md5 = '2cd0971a73f135ceb6aae74fe724a6f5'\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ProteinStructure.root","title":"root","text":"<pre><code>root = Path(root)\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ProteinStructure.target_transform","title":"target_transform","text":"<pre><code>target_transform = target_transform\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ProteinStructure.transform","title":"transform","text":"<pre><code>transform = transform\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.ProteinStructure.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.RoadNetwork","title":"RoadNetwork","text":"<pre><code>RoadNetwork(\n    root: str | Path = None,\n    transform: Callable | None = Lambda(\n        lambda x: (x - as_tensor([9.7318, 57.0838]))\n        / as_tensor([0.6273, 0.2895])\n    ),\n    target_transform: Callable | None = Lambda(\n        lambda y: (y - 22.1854) / 18.618\n    ),\n    download: bool = False,\n)\n</code></pre> <p>               Bases: <code>RegressionDataset</code></p> <p>3D Road Network (434,874 \u00d7 2).</p> <p>This UCI Dataset contains longitude, latitude and altitude values of a road network in North Jutland, Denmark (covering a region of 185x135 km2). Elevation values where extracted from a publicly available massive Laser Scan Point Cloud for Denmark. The regression task is to predict the altitude from longitude and latitude measurements.</p> <p>Source: https://archive.ics.uci.edu/ml/datasets/3D+Road+Network+(North+Jutland,+Denmark)</p> <p>Methods:</p> Name Description <code>download</code> <p>Attributes:</p> Name Type Description <code>URL</code> <code>filepath</code> <code>str</code> <code>md5</code> <code>root</code> <code>target_transform</code> <code>transform</code>"},{"location":"api/datasets/tabular/#inferno.datasets.RoadNetwork.URL","title":"URL","text":"<pre><code>URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00246/\"\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.RoadNetwork.filepath","title":"filepath","text":"<pre><code>filepath: str\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.RoadNetwork.md5","title":"md5","text":"<pre><code>md5 = '989a6f4574e09ee6735d8af2e5885cc1'\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.RoadNetwork.root","title":"root","text":"<pre><code>root = Path(root)\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.RoadNetwork.target_transform","title":"target_transform","text":"<pre><code>target_transform = target_transform\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.RoadNetwork.transform","title":"transform","text":"<pre><code>transform = transform\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.RoadNetwork.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.WineQuality","title":"WineQuality","text":"<pre><code>WineQuality(\n    root: str | Path = None,\n    transform: Callable | None = Lambda(\n        lambda x: (\n            x\n            - as_tensor(\n                [\n                    8.3196,\n                    0.5278,\n                    0.271,\n                    2.5388,\n                    0.0875,\n                    15.8749,\n                    46.4678,\n                    0.9967,\n                    3.3111,\n                    0.6581,\n                    10.423,\n                ]\n            )\n        )\n        / as_tensor(\n            [\n                1.7411,\n                0.17906,\n                0.1948,\n                1.4099,\n                0.047065,\n                10.46,\n                32.895,\n                0.0018873,\n                0.15439,\n                0.16951,\n                1.0657,\n            ]\n        )\n    ),\n    target_transform: Callable | None = Lambda(\n        lambda y: (y - 5.636) / 0.8076\n    ),\n    download: bool = False,\n    wine_type: Literal[\"red\", \"white\"] = \"red\",\n)\n</code></pre> <p>               Bases: <code>RegressionDataset</code></p> <p>Wine quality prediction from physicochemical properties (4,898 \u00d7 11).</p> <p>This UCI dataset contains red and white vinho verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests.</p> <p>Source: https://archive.ics.uci.edu/dataset/186/wine+quality</p> <p>Methods:</p> Name Description <code>download</code> <p>Attributes:</p> Name Type Description <code>URL</code> <code>filepath</code> <code>str</code> <code>md5</code> <code>root</code> <code>target_transform</code> <code>transform</code> <code>wine_type</code>"},{"location":"api/datasets/tabular/#inferno.datasets.WineQuality.URL","title":"URL","text":"<pre><code>URL = \"https://archive.ics.uci.edu/static/public/186/wine+quality.zip\"\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.WineQuality.filepath","title":"filepath","text":"<pre><code>filepath: str\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.WineQuality.md5","title":"md5","text":"<pre><code>md5 = {\n    \"red\": \"7d814a1bda02145efe703f4e1c01847a\",\n    \"white\": \"b56c9a78a7fcad87a58fc586bf5298bc\",\n}\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.WineQuality.root","title":"root","text":"<pre><code>root = Path(root)\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.WineQuality.target_transform","title":"target_transform","text":"<pre><code>target_transform = target_transform\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.WineQuality.transform","title":"transform","text":"<pre><code>transform = transform\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.WineQuality.wine_type","title":"wine_type","text":"<pre><code>wine_type = wine_type\n</code></pre>"},{"location":"api/datasets/tabular/#inferno.datasets.WineQuality.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre>"},{"location":"api/datasets/text/","title":"Text","text":""},{"location":"api/datasets/text/#inferno.datasets","title":"datasets &gt; Text Data","text":""},{"location":"examples/ibvi/classification/classification/","title":"Classification on Two Moons Toy Data","text":"<p>This toy example shows how to train a variational neural network for binary classification on the two moons dataset using implicit regularization via SGD initialized to the prior, as described in this paper. This approach avoids the computational cost of explicit regularization for non-trivial variational families and preserves beneficial inductive biases.</p> <p>You can run this example yourself via the corresponding standalone script.</p>"},{"location":"examples/ibvi/classification/classification/#data","title":"Data","text":"<p>We begin by generating synthetic training and test data based on the two moons classification technique.</p> <p></p>"},{"location":"examples/ibvi/classification/classification/#model","title":"Model","text":"<p>Next, we define a fully-connected stochastic neural network using a pre-defined <code>models.MLP</code>.</p> Model<pre><code>from torch import nn\n\nfrom inferno import bnn, loss_fns\n\nmodel = bnn.Sequential(\n    inferno.models.MLP(\n        in_size=2,\n        hidden_sizes=[hidden_width] * num_hidden_layers,\n        out_size=1,\n        activation_layer=nn.SiLU,\n        cov=[bnn.params.FactorizedCovariance()]\n        + [None] * (num_hidden_layers - 1)\n        + [bnn.params.FactorizedCovariance()],\n        bias=True,\n    ),\n    nn.Flatten(-2, -1),\n    parametrization=bnn.params.MUP(),\n)\n</code></pre> <ol> <li>PyTorch <code>nn.Module</code>s can be used as part of <code>inferno</code> models.</li> </ol>"},{"location":"examples/ibvi/classification/classification/#training","title":"Training","text":"<p>We train the model via the expected loss, \\( \\bar{\\ell}(\\theta) = \\mathbb{E}_{q_\\theta(w)}[\\ell(y, f_w(X))] \\) i.e. the average loss of the model when drawing weights from the variational distribution \\(q_\\theta(w)\\). In practice, for efficiency we only use a single sample per batch during training.</p> Training<pre><code># Loss function\nloss_fn = loss_fns.BCEWithLogitsLoss()\n\n# Optimizer\noptimizer = torch.optim.SGD(\n    params=model.parameters_and_lrs(\n        lr=lr, optimizer=\"SGD\"\n    ),  # Sets module-specific learning rates\n    lr=lr,\n    momentum=0.9,\n)\n\n# Training loop\nfor epoch in tqdm.trange(num_epochs):\n    model.train()\n\n    for X_batch, y_batch in iter(train_dataloader):\n        optimizer.zero_grad()\n\n        X_batch = X_batch.to(device=device)\n        y_batch = y_batch.to(device=device)\n\n        logits = model(X_batch)\n\n        loss = loss_fn(logits, y_batch)\n\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"examples/ibvi/classification/classification/#results","title":"Results","text":""},{"location":"examples/ibvi/classification/classification/#learning-curves","title":"Learning Curves","text":""},{"location":"examples/ibvi/classification/classification/#decision-boundary","title":"Decision Boundary","text":""},{"location":"examples/ibvi/regression/regression/","title":"Regression on Toy Data","text":"<p>This toy example shows how to train a variational neural network via implicit regularization via SGD initialized to the prior as described in this paper. This avoids the computational cost of regularization which for non-trivial variational families is often significant and preserves beneficial inductive biases.</p> <p>You can run this example yourself via the corresponding standalone script.</p>"},{"location":"examples/ibvi/regression/regression/#data","title":"Data","text":"<p>We begin by generating some synthetic training and test data.</p> <p></p>"},{"location":"examples/ibvi/regression/regression/#model","title":"Model","text":"<p>Next, we define a fully-connected stochastic neural network using a pre-defined <code>models.MLP</code>.</p> Model<pre><code>from torch import nn\n\nfrom inferno import bnn, loss_fns\n\nmodel = bnn.Sequential(\n    inferno.models.MLP(\n        in_size=1,\n        hidden_sizes=[hidden_width] * num_hidden_layers,\n        out_size=1,\n        activation_layer=nn.SiLU,  # (1)!\n        cov=[bnn.params.FactorizedCovariance()]\n        + [None] * (num_hidden_layers - 1)\n        + [bnn.params.FactorizedCovariance()],\n        bias=True,\n    ),\n    nn.Flatten(-2, -1),\n    parametrization=bnn.params.MUP(),\n)\n</code></pre> <ol> <li>PyTorch <code>nn.Module</code>s can be used as part of <code>inferno</code> models.</li> </ol>"},{"location":"examples/ibvi/regression/regression/#training","title":"Training","text":"<p>We train the model via the expected loss,  \\( \\bar{\\ell}(\\theta) = \\mathbb{E}_{q_\\theta(w)}[\\ell(y, f_w(X))] \\) i.e. the average loss of the model when drawing weights from the variational distribution \\(q_\\theta(w)\\). In practice, for efficiency we only use a single sample per batch during training.</p> Training<pre><code># Loss function\nloss_fn = loss_fns.MSELoss()\n\n# Optimizer\noptimizer = torch.optim.SGD(\n    params=model.parameters_and_lrs(\n        lr=lr, optimizer=\"SGD\"\n    ),  # Sets module-specific learning rates\n    lr=lr,\n    momentum=0.9,\n)\n\n# Training loop\nfor epoch in tqdm.trange(num_epochs):\n    model.train()\n\n    for X_batch, y_batch in iter(train_dataloader):\n        optimizer.zero_grad()\n\n        X_batch = X_batch.to(device=device)\n        y_batch = y_batch.to(device=device)\n\n        y_batch_pred = model(X_batch)  # Uses a single parameter sample\n\n        loss = loss_fn(y_batch_pred, y_batch)\n\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"examples/ibvi/regression/regression/#results","title":"Results","text":""},{"location":"examples/ibvi/regression/regression/#learning-curves","title":"Learning Curves","text":""},{"location":"examples/ibvi/regression/regression/#prediction","title":"Prediction","text":""}]}